{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Chapter 7*<br> Trial-and-Error Searching & Data Fitting  \n",
    "\n",
    "| | | |\n",
    "|:---:|:---:|:---:|\n",
    "| ![image](Figs/Cover.png)|[From **COMPUTATIONAL PHYSICS**, 3rd Ed, 2015](http://physics.oregonstate.edu/~rubin/Books/CPbook/index.html) <br>RH Landau, MJ Paez, and CC Bordeianu (deceased) <br>Copyrights: <br> [Wiley-VCH, Berlin;](http://www.wiley-vch.de/publish/en/books/ISBN3-527-41315-4/) and [Wiley & Sons, New York](http://www.wiley.com/WileyCDA/WileyTitle/productCd-3527413154.html)<br>  R Landau, Oregon State Unv, <br>MJ Paez, Univ Antioquia,<br> C Bordeianu, Univ Bucharest, 2015.<br> Support by National Science Foundation.|![image](Figs/BackCover.png)|\n",
    "    \n",
    "**7 Trial-and-Error Searching & Data Fitting**<br>\n",
    "    [7.1 Problem 1: A Search for Quantum States in a Box](#7.1)<br>\n",
    "    [7.2 Algorithm: Trial-and-Error Roots via Bisection](#7.1)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.2.1 Implementation: Bisection Algorithm](#7.2.1)<br>\n",
    "    [7.3 Improved Algorithm: Newton-Raphson Searching](#7.3)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.3.1  Newton-Raphson with Backtracking](#7.3.1)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.3.2 Implementation: Newton-Raphson Algorithm](#7.3.2)<br>\n",
    "    [7.4 Problem 2: Temperature Dependence of Magnetization](#7.4)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.4.1 Searching Exercise](#7.4.1)<br>\n",
    "    [7.5 Problem 3: Fitting An Experimental Spectrum](#7.5)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.5.1 Lagrange Implementation, Assessment](#7.5.1)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.5.2 Cubic Spline Interpolation](#7.5.2)<br>\n",
    "    [7.6 Problem 4:Fitting Exponential Decay](#7.6)<br>\n",
    "    [7.7 Least-Squares Fitting (Theory)](#7.7)<br>\n",
    "    [7.7.1 Theory and Implementation](#7.7.1)<br>\n",
    "    [7.8 Exercises: Fitting Exponential Decay, Heat Flow & Hubble’s Law](#7.8)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.8.1 Linear Quadratic Fit](#7.8.1)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;[7.8.2 Problem 5: Nonlinear Fit to a Breit-Wigner](#7.8.2)<br>\n",
    "\n",
    "\n",
    "*In this chapter we add more tool to our computational toolbox. First we\n",
    "devise ways to solve equations via a trial and error search, sometimes\n",
    "using our new-found numerical differentiation tools. Although\n",
    "trial-and-error searching may not sound very precise, it is in fact\n",
    "widely used to solve problems where analytic solutions do not exist or\n",
    "are not practical. We have already looked at one such example in\n",
    "[Chapter 6](CP06.ipynb), where we saw how the two-weights-on-a-string\n",
    "problem led to matrix equations.*\n",
    "\n",
    "In [Chapter 8, *Solving Differential Equations; Nonlinear Oscillations*](CP08.ipynb), we combine trial-and-error searching with\n",
    "the solution of ordinary differential equations to solve the general\n",
    "quantum eigenvalue problem. The second half of the present chapter\n",
    "introduces some aspects of data fitting. We examine how to interpolate\n",
    "within a table of numbers and how to do a least-squares fit of a\n",
    "function to data, the latter often requiring a search. Data fitting is\n",
    "an art worthy of serious study by all true scientists. While our\n",
    "coverage is hardly adequate for that.\n",
    "\n",
    "Although this chapter is mainly math, it is important for the readers to\n",
    "experience how programs can act intelligently and modify their behavior\n",
    "based on what has happened before (the programs, that is). This shows\n",
    "how not all programs are purely deterministic. It is also important to\n",
    "see how programs can fail, to see just how they fail, and then to see\n",
    "how an improved algorithm may be both more robust and faster. The latter\n",
    "point being that throwing more computer power at a problem is hardly\n",
    "ever as wise as using an improved algorithm.\n",
    "\n",
    "** This Chapter’s Lecture, Slide Web Links, Applets & Animations**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "|[All Lectures](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/index.html)|[![anything](Figs/RHLlectureMod4.png)](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/index.html)|\n",
    "\n",
    "| *Lecture (Flash)*| *Slides* | *Sections*|*Lecture (Flash)*| *Slides* | *Sections*|  \n",
    "|- - -|:- - -:|:- - -:|- - -|:- - -:|:- - -:|\n",
    "|[Numerical Differentiation](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Differentiation/Differentiation.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Differentiate.pdf)|7.1-7.6|[Trial and Error Searching](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Searching/Searching.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Trial_Err.pdf)|7.7-7.10 |\n",
    "| [N-Dimensional Searching](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/NdimSearch/NdimSearch.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/NDsearch.pdf)|8.2 |[Matrix Compute](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Matrices/Matrices.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Matrix.pdf)|8.1|\n",
    "|[Matrix Compute II](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Matrices/Matrices.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Matrix2.pdf)|8.4|[Trial and Error Searching](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Searching/Searching.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Trial_Err.pdf)|7.7-7.10|\n",
    "|[N-Dimensional Searching](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/NdimSearch/NdimSearch.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/NDsearch.pdf)|8.2.2|[Interpolation](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Interpolation/Interpolation.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Interp.pdf)|8.5|\n",
    "|[Least Square Fitting](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Fitting/Fitting.html)|[pdf](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/LeastSqFit.pdf)| 8.7 |Applet: [Lagrange Interpolation](http://science.oregonstate.edu/~rubin/Books/CPbook/eBook/Applets/index.html)|-| 8.5 |\n",
    "|Applet:  [Spline Interpolation](http://science.oregonstate.edu/~rubin/Books/CPbook/eBook/Applets/index.html)|-|8.5||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1  Problem 1: A Search for Quantum States in a Box<a id=\"7.1\"></a>\n",
    "\n",
    "[![image](Figs/RHLlectureMod4.png)](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Searching/Searching.html)\n",
    "\n",
    "Many computer techniques are well-defined sets of procedures leading to\n",
    "definite outcomes. In contrast, some computational techniques are\n",
    "trial-and-error in which decisions on what path to follow are made based\n",
    "on the current values of variables, and the program quits only when it\n",
    "thinks it has solved the problem. (We already did some of this when we\n",
    "summed a power series until the terms became small.) Writing this type\n",
    "of program is usually interesting because we must foresee how to have\n",
    "the computer act intelligently in all possible situations, and running\n",
    "them is very much like an experiment in which it is hard to predict what\n",
    "the computer will come up with.\n",
    "\n",
    "**Problem:** Probably the most standard problem in quantum\n",
    "mechanics is to solve for the energies of a particle\n",
    "of mass *m* bound within a 1-D square well of radius *a*:\n",
    "\n",
    "$$\\tag*{7.1}\n",
    "    V(x) =\\begin{cases}\n",
    "    -V_0, & \\mbox{for } |x| \\leq a,    \\\\\n",
    "  0, & \\mbox{for }  |x| \\geq a.\n",
    "  \\end{cases}$$\n",
    "  \n",
    "\\[*Note:* We solve this same problem in § 9.1 using an approach\n",
    "that is applicable to almost any potential and which also provides the\n",
    "wave functions. The approach of this section works only for the eigen\n",
    "energies of a square well.\\]\n",
    "As shown in quantum mechanics texts \\[[Gottfried(66)](BiblioLinked.html#gott)\\], the energies of\n",
    "the bound states *E* = −*E*<sub>*B*</sub> &lt; 0 within this well are\n",
    "solutions of the transcendental equations\n",
    "\n",
    "$$\\begin{align}\n",
    " \\tag*{7.2}\n",
    "\\sqrt{10-E_B}  \\tan\\left(\\sqrt{10-E_B}\\right) & =  \\sqrt{E_B}\n",
    "\\quad\n",
    "\\mbox{(even)},\\\\\n",
    "\\sqrt{10-E_B}\\ \\ \\mbox{cotan}\\left(\\sqrt{10-E_B}\\right)& =\n",
    "\\sqrt{E_B}\\quad \\mbox{(odd)},\\tag*{7.3}\\end{align}$$\n",
    "\n",
    "where even and odd refer to the symmetry of the wave function. Here we\n",
    "have chosen units such that ℏ = 1, 2*m* = 1, *a* = 1, and\n",
    "*V*<sub>0</sub> = 10. Your **problem** is to\n",
    "\n",
    "1.  Find several bound-state energies *E*<sub>*B*</sub> for even wave\n",
    "    functions, that is, the solution of (7.2).\n",
    "\n",
    "2.  See if making the potential deeper, say, by changing the 10 to a 20\n",
    "    or a 30, produces a larger number of, or deeper, bound states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Figs/Fig7_1.png)\n",
    "\n",
    "**Figure 7.1** A graphical representation of the steps involved in solving for a\n",
    "zero of *fx* using the bisection algorithm. The bisection algorithm takes the\n",
    "midpoint of the interval as the new guess for *x*, and so each step reduces the\n",
    "interval size by one-half. Four steps are shown for the algorith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection.py, Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it  0  x  3.5  f(x)  -5.37291337458\n",
      "it  1  x  1.75  f(x)  -2.1064921113\n",
      "it  2  x  0.875  f(x)  0.406993716327\n",
      "it  3  x  1.3125  f(x)  -0.801632466222\n",
      "it  4  x  1.09375  f(x)  -0.175435456215\n",
      "it  5  x  0.984375  f(x)  0.12239260298\n",
      "it  6  x  1.0390625  f(x)  -0.0250054227653\n",
      "it  7  x  1.01171875  f(x)  0.0490901385459\n",
      "it  8  x  1.025390625  f(x)  0.012139324015\n",
      "it  9  x  1.0322265625  f(x)  -0.00640908122177\n",
      "it  10  x  1.02880859375  f(x)  0.00287114769538\n",
      "it  11  x  1.03051757812  f(x)  -0.00176746446558\n",
      "it  12  x  1.02966308594  f(x)  0.000552217724286\n",
      "it  13  x  1.03009033203  f(x)  -0.00060752941015\n",
      "it  14  x  1.02987670898  f(x)  -2.76323444508e-05\n",
      "it  15  x  1.02976989746  f(x)  0.000262298565582\n",
      "it  16  x  1.02982330322  f(x)  0.000117334579351\n",
      "it  17  x  1.0298500061  f(x)  4.48514846303e-05\n",
      "it  18  x  1.02986335754  f(x)  8.60966188276e-06\n",
      "it  19  x  1.02987003326  f(x)  -9.51131833604e-06\n",
      "it  20  x  1.0298666954  f(x)  -4.50822489562e-07\n",
      "it  21  x  1.02986502647  f(x)  4.0794211309e-06\n",
      "it  22  x  1.02986586094  f(x)  1.81429967916e-06\n",
      "\n",
      " root found with precision eps =  1e-06\n",
      "Root = 1.02986586094\n"
     ]
    }
   ],
   "source": [
    "# Bisection.py, Notebook Version\n",
    "\n",
    "from __future__ import  print_function\n",
    "from numpy import *\n",
    "\n",
    "def f(x):                                      # Function\n",
    "    return 2*cos(x)-x\n",
    "\n",
    "def bisection(xminus,xplus,Nmax,eps):          # x-,x+ Nmax error\n",
    "    for it in range(0,Nmax):\n",
    "        x=(xminus+xplus)/2                     # Mid point\n",
    "        print(\"it \",it, \" x \", x, \" f(x) \",f(x))\n",
    "        if (f(xplus)*f(x))>0:                      # Root in other half\n",
    "            xplus=x                            # Change x+ to x\n",
    "        else:\n",
    "            xminus=x                           # Change x- to x\n",
    "        if(xplus-xminus<eps):                           # Converged?\n",
    "            print(\"\\n root found with precision eps = \",eps)\n",
    "            break\n",
    "        if it==Nmax-1:\n",
    "             print(\"\\n root not found after Nmax iterations \")   \n",
    "    return x  \n",
    "\n",
    "eps=1e-6\n",
    "a=0.0\n",
    "b=7.0\n",
    "Nmax=100\n",
    "root=bisection(a,b,Nmax,eps)\n",
    "print(\"Root =\",root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2  Algorithm: Trial-and-Error Roots via Bisection<a id=\"7.2\"></a>\n",
    "\n",
    "Trial-and-error root finding looks for a value of *x* for which $$\\tag*{7.4} f(x)\n",
    "\\simeq 0,$$\n",
    "\n",
    "where the 0 on the right-hand side (RHS) is conventional (an equation\n",
    "such as 10sin*x* = 3*x*<sup>3</sup> can easily be written as\n",
    "10sin*x* − 3*x*<sup>3</sup> = 0). The search procedure starts with a\n",
    "guessed value for *x*, substitutes that guess into *f*(*x*) (the\n",
    "“trial”), and then sees how far the LHS is from zero (the “error”). The\n",
    "program then changes *x* based on the error and tries out the new guess\n",
    "in *f*(*x*). The procedure continues until *f*(*x*)≃0 to some desired\n",
    "level of precision, or until the changes in *x* are insignificant, or if\n",
    "the search seems endless.\n",
    "\n",
    "The most elementary trial-and-error technique is the *bisection\n",
    "algorithm*. It is reliable but slow. If you know some interval in which\n",
    "*f*(*x*) changes sign, then the bisection algorithm will always converge\n",
    "to the root by finding progressively smaller and smaller intervals\n",
    "within which the zero lies. Other techniques, such as the Newton-Raphson\n",
    "method we describe next, may converge more quickly, but if the initial\n",
    "guess is not close, it may become unstable and fail completely.\n",
    "\n",
    "The basis of the bisection algorithm is shown in Figure 7.1. We start\n",
    "with two values of *x* between which we know a zero occurs. (You can\n",
    "determine these by making a graph or by stepping through different *x*\n",
    "values and looking for a sign change.) To be specific, let us say that\n",
    "*f*(*x*) is negative at *x*<sub>−</sub> and positive at *x*<sub>+</sub>:\n",
    "\n",
    "$$\\tag*{7.5}\n",
    "    f(x_-) \\lt; 0, \\quad f(x_+) \\gt; 0.$$\n",
    "\n",
    "(Note that it may well be that *x*<sub>−</sub> &gt; *x*<sub>+</sub> if\n",
    "the function changes from positive to negative as *x* increases.) Thus\n",
    "we start with the interval *x*<sub>+</sub> ≤ *x* ≤ *x*<sub>−</sub>\n",
    "within which we know a zero occurs. The algorithm (implemented in\n",
    "`Bisection.py`) then picks the new *x* as the bisection of the interval\n",
    "and selects as its new interval the half in which the sign change\n",
    "occurs:\n",
    "\n",
    "       x = ( xPlus + xMinus ) / 2\n",
    "       if ( f(x) f(xPlus) > 0 ) xPlus = x\n",
    "        else xMinus = x\n",
    "\n",
    "\n",
    "This process continues until the value of *f*(*x*) is less than a\n",
    "predefined level of precision or until a predefined (large) number of\n",
    "subdivisions occurs.\n",
    "\n",
    "[ **Listing 7.1 .py**](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/Bisection.py) is a simple implementation of the bisection\n",
    "algorithm for finding a zero of a function, in this case\n",
    "2cos*x* − *x*.\n",
    "\n",
    "The example in Figure 7.1 shows the first interval extending from\n",
    "*x*<sub>−</sub> = *x*<sub>+1</sub> to\n",
    "*x*<sub>+</sub> = *x*<sub>−1</sub>. We bisect that interval at *x*, and\n",
    "because *f*(*x*)&lt;0 at the midpoint, we set\n",
    "*x*<sub>−</sub> ≡ *x*<sub>−2</sub> = *x* and label it *x*<sub>−2</sub>\n",
    "to indicate the second step. We then use\n",
    "*x*<sub>+2</sub> ≡ *x*<sub>+1</sub> and *x*<sub>−2</sub> as the next\n",
    "interval and continue the process. We see that only *x*<sub>−</sub>\n",
    "changes for the first three steps in this example, but that for the\n",
    "fourth step *x*<sub>+</sub> finally changes. The changes then become too\n",
    "small for us to show.\n",
    "\n",
    "### 7.2.1  Implementation: Bisection Algorithm<a id=\"7.2.1\"></a>\n",
    "\n",
    "1.  The first step in implementing any search algorithm is to get an\n",
    "    idea of what your function looks like. For the present problem you\n",
    "    do this by making a plot of $f(E)=\\sqrt{10 -\n",
    "    E_B} \\tan(\\sqrt{10-E_B}) -\\sqrt{E_B}$ <span>*versus*</span>\n",
    "    *E*<sub>*B*</sub>. Note from your plot some approximate values at\n",
    "    which *f*(*E*<sub>*B*</sub>)=0. Your program should be able to find\n",
    "    more exact values for these zeros.\n",
    "\n",
    "2.  Write a program that implements the bisection algorithm and use it\n",
    "    to find some solutions of (7.2).\n",
    "\n",
    "3.  *Warning:* Because the tan function has singularities, you have to\n",
    "    be careful. In fact, your graphics program (or Maple) may not\n",
    "    function accurately near these singularities. One cure is to use a\n",
    "    different but equivalent form of the equation. Show that an\n",
    "    equivalent form of (7.2) is\n",
    "\n",
    "    $$\\tag*{7.6}\n",
    "        \\sqrt{E} \\cot(\\sqrt{10-E}) - \\sqrt{10-E} =0.$$\n",
    "\n",
    "4.  Make a second plot of (7.6), which also has singularities but at\n",
    "    different places. Choose some approximate locations for zeros from\n",
    "    this plot.\n",
    "\n",
    "5.  Evaluate *f*(*E*<sub>*B*</sub>) and thus determine directly the\n",
    "    precision of your solution.\n",
    "\n",
    "6.  Compare the roots you find with those given by <span>Maple</span> or\n",
    "    <span>Mathematica</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3   Improved Algorithm: Newton-Raphson Searching<a id=\"7.3\"></a>\n",
    "\n",
    "The Newton-Raphson algorithm finds approximate roots of the equation\n",
    "\n",
    "$$\\tag*{7.7}\n",
    "    f(x) =0$$\n",
    "\n",
    "more quickly than the bisection method. As we see graphically in Figure 7.1, this\n",
    "algorithm is the equivalent of drawing a straight line *f*(*x*)≃*mx* + *b*\n",
    "tangent to the curve at an *x* value for which *f*(*x*)≃0 and then using the\n",
    "intercept of the line with the *x* axis at *x* = −*b*/*m* as an improved guess\n",
    "for the root. If the “curve” were a straight line, the answer would be exact;\n",
    "otherwise, it is a good approximation if the guess is close enough to the root for\n",
    "*f*(*x*) to be nearly linear. The process continues until some set level of\n",
    "precision is reached. If a guess is in a region where *f*(*x*) is nearly linear\n",
    "(Figure 7.1), then the convergence is much more rapid than for the bisection\n",
    "algorithm.\n",
    "\n",
    "The analytic formulation of the Newton-Raphson algorithm starts with an old\n",
    "guess *x*<sub>0</sub> and expresses a new guess *x* as a correction *Δx* to\n",
    "the old guess:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.8.xml)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.8}\n",
    "  x_0 & =  \\mbox{old guess},\\quad\n",
    "  \\Delta x = \\mbox{unknown correction} \\\\\n",
    "    \\Rightarrow\\quad x & =  x_0 + \\Delta x = \\mbox{(unknown) new guess}.\\tag*{7.9}\n",
    "   \\end{align}$$\n",
    "\n",
    "We next expand the known function *f*(*x*) in a Taylor series around\n",
    "*x*<sub>0</sub> and keep only the linear terms:\n",
    "\n",
    "$$\\tag*{7.10} f(x = x_0 + \\Delta x) \\simeq f(x_0) + \\left. \\frac{d f}\n",
    "{dx}\\right|_{x_0} \\Delta x.$$\n",
    "\n",
    "We then determine the correction *Δx* by calculating the point at which this\n",
    "linear approximation to *f*(*x*) crosses the *x* axis:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.11}\n",
    "  f(x_0) + \\left. \\frac{d f} {dx}\\right|_{x_0} \\Delta x  & = 0 ,\\\\\n",
    " \\Rightarrow  \\quad   \\Delta x & = - \\frac{f(x_0)} {  \\left. {df /\n",
    "    dx}\\right|_{x_0}}.  \\tag*{7.12}\\end{align}$$\n",
    "\n",
    "The procedure is repeated starting at the improved *x* until some set\n",
    "level of precision is obtained.\n",
    "\n",
    "![image](Figs/Fig7_2.png)\n",
    "\n",
    "**Figure 7.2** A graphical representation of the steps involved in solving for a\n",
    "zero of *fx* using the Newton-Raphson method. The Newton-Raphson method\n",
    "takes the new guess as the zero of the line tangent to *fx* at the old guess. Two\n",
    "guesses are shown.\n",
    "\n",
    "The Newton-Raphson algorithm (7.12) requires evaluation of the derivative\n",
    "*df*/*dx* at each value of *x*<sub>0</sub>. In many cases you may have an\n",
    "analytic expression for the derivative and can build it into the algorithm.\n",
    "However, especially for more complicated problems, it is simpler and less\n",
    "error-prone to use a numerical forward-difference approximation to the\n",
    "derivative:\\[*Note:* We discuss numerical differentiation in\n",
    "[Chapter 5.](CP05.ipynb)\\]\n",
    "\n",
    "$$\\tag*{7.13}\n",
    "\\frac{d f} {d x} \\simeq \\frac{f (x + \\delta x) - f (x)}{\\delta x},$$\n",
    "\n",
    "where *δx* is some small change in *x* that you just chose \\[different from the\n",
    "*Δ* used for searching in (7.12)\\]. While a central-difference approximation for\n",
    "the derivative would be more accurate, it would require additional evaluations\n",
    "of the *f*’s, and once you find a zero, it does not matter how you got there. In\n",
    "Listing 7.2 we give a program `NewtonCD.py` that implement the search with\n",
    "the central difference derivative.\n",
    "\n",
    "[**Listing 7.2  NewtonCD.py**](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/NewtonCD.py) uses the Newton-Raphson method to search for\n",
    "a zero of the function *fx*. A central-difference approximation is used to\n",
    "determine *df*/*dx*.\n",
    "\n",
    "![image](Figs/Fig7_3.png)\n",
    "\n",
    " **Figure 7.3** Two examples of how the\n",
    "Newton-Raphson algorithm may fail if the initial guess is not in the\n",
    "region where *fx* can be approximated by a straight line. *Top:* A guess\n",
    "lands at a local minimum/maximum, that is, a place where the derivative\n",
    "vanishes, and so the next guess ends up at *x* = ∞. *Bottom:* The search\n",
    "has fallen into an infinite loop. The technique know as “backtracking”\n",
    "could eliminate this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NewtonCD.py, Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # =  0  x =  2.0  f(x) =  -2.83229367309\n",
      "Iteration # =  1  x =  0.995137139436  f(x) =  0.0936385754551\n",
      "Iteration # =  2  x =  1.03104193456  f(x) =  -0.00319130209877\n",
      "Root found, tolerance eps =  1e-06\n",
      "Root = 1.02986675107\n"
     ]
    }
   ],
   "source": [
    "# NewtonCD.py, Notebook Version,  Newton search with central difference\n",
    "\n",
    "from __future__ import  print_function\n",
    "from numpy import *\n",
    "\n",
    "x = 2.;         dx = 1e-2;        eps = 1e-6;                            # Parameters\n",
    "imax = 100;                                                    # Max no of iterations\n",
    "def f(x):                                                              # function def\n",
    "    return 2*cos(x) - x\n",
    "def Newton(x,dx,eps):\n",
    "    for it in range( 0, imax + 1):\n",
    "        F = f(x)\n",
    "        if ( abs(F) <= eps ):                                     # Check for convergence\n",
    "            print(\"Root found, tolerance eps = \" , eps) \n",
    "            break\n",
    "        print(\"Iteration # = \", it, \" x = \", x, \" f(x) = \", F)\n",
    "        df = ( f(x + dx/2)  -  f(x - dx/2) )/dx                      # Central diff deriv\n",
    "        dx = - F/df \n",
    "        x   += dx      \n",
    "    return x   \n",
    "root=Newton(x,dx,eps)\n",
    "print(\"Root =\",root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1  Newton-Raphson with Backtracking<a id=\"7.3.1\"></a>\n",
    "\n",
    "Two examples of possible problems with the Newton-Raphson algorithm are\n",
    "shown in Figure 7.3. On the left we see a case where the search takes us to an\n",
    "*x* value where the function has a local minimum or maximum, that is, where\n",
    "*df*/*dx* = 0. Because *Δx* = −*f*/*f*′, this leads to a horizontal tangent\n",
    "(division by zero), and so the next guess is *x* = ∞, from where it is hard to\n",
    "return. When this happens, you need to start your search with a different guess\n",
    "and pray that you do not fall into this trap again. In cases where the correction\n",
    "is very large but maybe not infinite, you may want to try backtracking (described\n",
    "below) and hope that by taking a smaller step you will not get into as much\n",
    "trouble.\n",
    "\n",
    "In Figure 7.3 on the right we see a case where a search falls into an infinite loop\n",
    "surrounding the zero without ever getting there. A solution to this problem is\n",
    "also called *backtracking*. As the name implies, in cases where the new guess\n",
    "*x*<sub>0</sub> + *Δx* leads to an increase in the magnitude of the function,\n",
    "|*f*(*x*<sub>0</sub> + *Δx*)|<sup>2</sup> &gt; |*f*(*x*<sub>0</sub>)|<sup>2</sup>,\n",
    "you can backtrack somewhat and try a smaller guess, say,\n",
    "*x*<sub>0</sub> + *Δx*/2. If the magnitude of *f* still increases, then you just\n",
    "need to backtrack some more, say, by trying *x*<sub>0</sub> + *Δx*/4 as your\n",
    "next guess, and so forth. Because you know that the tangent line leads to a local\n",
    "decrease in |*f*|, eventually an acceptable small enough step should be found.\n",
    "\n",
    "The problem in both these cases is that the initial guesses were not\n",
    "close enough to the regions where *f*(*x*) is approximately linear. So\n",
    "again, a good plot may help produce a good first guess. Alternatively,\n",
    "you may want to start your search with the bisection algorithm and then\n",
    "switch to the faster Newton-Raphson algorithm when you get closer to the\n",
    "zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2  Implementation: Newton-Raphson Algorithm<a id=\"7.3.2\"></a>\n",
    "\n",
    "1.  Use the Newton-Raphson algorithm to find some energies\n",
    "    *E*<sub>*B*</sub> that are solutions of (7.2). Compare these\n",
    "    solutions to the ones found with the bisection algorithm.\n",
    "\n",
    "2.  Again, notice that the 10 in (7.2) is proportional to the strength\n",
    "    of the potential that causes the binding. See if making the\n",
    "    potential deeper, say, by changing the 10 to a 20 or a 30, produces\n",
    "    more or deeper bound states. (Note that in contrast to the bisection\n",
    "    algorithm, your initial guess must be closer to the answer for the\n",
    "    Newton-Raphson algorithm to work.)\n",
    "\n",
    "3.  Modify your algorithm to include backtracking and then try it out on\n",
    "    some difficult cases.\n",
    "\n",
    "4.  Evaluate *f*(*E*<sub>*B*</sub>) and thus determine directly the\n",
    "    precision of your solution.\n",
    "\n",
    "![image](Figs/Fig7_4.png)\n",
    "\n",
    " **Figure 7.4** A function of the reduced\n",
    "magnetism *m* at three reduced temperatures *t*. A zero of this function\n",
    "determines the value of the magnetism at a particular value of *t*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4  Problem 2: Temperature Dependence of Magnetization<a id=\"7.4\"></a>\n",
    "\n",
    "**Problem:** Determine *M*(*T*) the magnetization as a function of\n",
    "temperature for simple magnetic materials.\n",
    "\n",
    "A collection of *N* spin 1/2 particles each with magnetic moment *μ* is at\n",
    "temperature *T*. The collection has an external magnetic field *B* applied to it\n",
    "and comes to equilibrium with *N*<sub>*L*</sub> particles in the lower energy\n",
    "state (spins aligned with the magnetic field), and with *N*<sub>*U*</sub>\n",
    "particles in the upper energy state (spins opposed to the magnetic field). The\n",
    "Boltzmann distribution law tells us that the relative probability of a state with\n",
    "energy *E* is proportional to exp(−*E*/(*k*<sub>*B*</sub>*T*)) where\n",
    "*k*<sub>*B*</sub> is Boltzmann’s constant. For a dipole with moment *μ*, its\n",
    "energy in a magnetic field is given by the dot product *E* = −μ ⋅ B. Accordingly\n",
    "spin up particle have lower energy in a magnetic field than spin down particles,\n",
    "and thus are more probable.\n",
    "\n",
    "Applying the Boltzmann distribution to our spin problem \\[[Kittel(05)](BiblioLinked.html#kit)\\],\n",
    "we have that the number of particles in the lower energy level (spin up)\n",
    "is\n",
    "\n",
    "$$\\tag*{7.14} N_L = N\\frac{e^{\\mu B/(k_B T)}}{e^{\\mu B/(k_B T)}+ e^{-\\mu\n",
    "B/(k_B T)}} ,$$\n",
    "\n",
    "while the number of particles in the upper energy level (spin down) is\n",
    "\n",
    "$$\\tag*{7.15}\n",
    " N_U = N\\frac{e^{-\\mu B/(k_B T)}}{e^{\\mu B/(k_B T)}+ e^{-\\mu B/(k_B T)}} .$$\n",
    "\n",
    "We now assume that the molecular magnetic field *B* = *λM* is much larger\n",
    "than the applied magnetic field and so replace *B* by the molecular field. This\n",
    "permits us to eliminate *B* from the preceding equations. The $$\\begin{align}\n",
    "\\tag*{7.39}\n",
    "\\frac{dN(t)}{dt} \\simeq -\\lambda N(t) = \\frac{1}{\\tau}N(t).\\end{align}$$\n",
    " This differential equation has an exponential solution for the number\n",
    "as well as for the decay rate:\n",
    "\n",
    "$$\\tag*{7.40} N(t) = N_{0} e^{-t/\\tau}, \\quad \\frac{dN(t)}{dt} = -\n",
    "\\frac{N_{0}} { \\tau} e^{-t/\\tau} =\\frac{dN(0)}{dt} e^{-t/\\tau}.$$\n",
    "\n",
    "Equation (7.40) is the theoretical formula we wish to “fit” to the data\n",
    "in Figure 7.6. The output of such a fit is a “best value” for the\n",
    "lifetime *τ*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5  Problem 3: Fitting An Experimental Spectrum<a id=\"7.5\"></a>\n",
    "\n",
    "[![image](Figs/RHLlectureMod4.png)](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Fitting/Fitting.html)\n",
    "\n",
    "*Data fitting is an art worthy of serious study by all scientists\n",
    "\\[[Bevington & Robinson(02)](BiblioLinked.html#bev)\\]. In the sections to follow we just scratch\n",
    "the surface by examining how to interpolate within a table of numbers\n",
    "and how to do a least-squares fit to data. We also show how to go about\n",
    "making a least-squares fit to nonlinear functions using some of the\n",
    "search techniques and subroutine libraries we have already discussed.*\n",
    "\n",
    "**Problem:** The cross sections measured for the resonant scattering of\n",
    "neutrons from a nucleus are given in Table 7.1. Your **problem** is to\n",
    "determine values for the cross sections at energy values lying between\n",
    "those in the table.\n",
    "\n",
    "You can solve this **problem** in a number of ways. The simplest is to\n",
    "numerically interpolate between the values of the experimental $f(E_i)$\n",
    "given in Table 7.1. This is direct and easy but does not account for\n",
    "there being experimental noise in the data. A more appropriate solution\n",
    "(discussed in § 7.7) is to find the *best fit* of a theoretical function\n",
    "to the data.We start with what we believe to be the “correct”\n",
    "theoretical description of the data,\n",
    "\n",
    "$$\\tag*{7.21}\n",
    "f(E) = \\frac{f_r} {(E-E_r)^{2} + \\Gamma^{2}/4},$$\n",
    "\n",
    "where $f_r, E_r$, and $\\Gamma$ are unknown parameters. We then adjust\n",
    "the parameters to obtain the best fit. This is a best fit in a\n",
    "statistical sense but in fact may not pass through all (or any) of the\n",
    "data points. For an easy, yet effective, introduction to statistical\n",
    "data analysis, we recommend \\[[Bevington & Robinson(02)](BiblioLinked.html#bev)\\].\n",
    "\n",
    "These two techniques of interpolation and least-squares fitting are\n",
    "powerful tools that let you treat tables of numbers as if they were\n",
    "analytic functions and sometimes let you deduce statistically meaningful\n",
    "constants or conclusions from measurements. In general, you can view\n",
    "data fitting as *global* or *local*. In global fits, a single function\n",
    "in $x$ is used to represent the entire set of numbers in a table like\n",
    "Table 7.1. While it may be spiritually satisfying to find a single\n",
    "function that passes through all the data points, if that function is\n",
    "not the correct function for describing the data, the fit may show\n",
    "nonphysical behavior (such as large oscillations) between the data\n",
    "points. The rule of thumb is that if you must interpolate, keep it local\n",
    "and view global interpolations with a critical eye.\n",
    "\n",
    "\n",
    "**Table 7.1** Experimental values for a scattering cross section ($f(E)$\n",
    "in the theory), each with absolute error $\\pm\\sigma_i$, as a function of\n",
    "energy ($x_i$ in the theory).\n",
    "\n",
    "|$i=$ | 1|2|3|4|5|6|7|8|9|\n",
    "|- - -|- - -|- - -|- - -|- - -|- - -|- - -|- - -|- - -|- - -|\n",
    "|$E_i$ (MeV) |0|25|50|75|100|125|150|175|200|\n",
    "| $g(E_i)$(mb)|10.6|16.0|45.0|83.5|52.8|19.9|10.8|8.25|4.7|\n",
    "| Error (mb)|9.34|17.9 |41.5 |85.5|51.5|21.5|10.8 |6.29 |4.14|\n",
    "\n",
    "[![image](Figs/RHLlectureMod4.png)](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Interpolation/Interpolation.html)\n",
    "\n",
    "Consider Table 7.1 as ordered data that we wish to interpolate. We call\n",
    "the independent variable $x$ and its tabulated values\n",
    "$x_i (i= 1,2, \\ldots)$, and assume that the dependent variable is the\n",
    "function $g(x)$, with tabulated values $g_i = g(x_i)$. We assume that\n",
    "$g(x)$ can be approximated as a ($n-1$)-degree polynomial in each\n",
    "interval $i$:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.23.xml) \n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.23}\n",
    "g_i(x) \\simeq a_0 + a_1 x + a_2 x^2 + \\cdots + a_{n-1}x^{n-1},\n",
    "\\quad(x \\simeq x_i).\\end{align}$$ \n",
    "\n",
    "Because our fit is local, we do not\n",
    "assume that one $g(x)$ can fit all the data in the table but instead use\n",
    "a different polynomial, that is, a different set of $a_i$ values, for\n",
    "each interval. While each polynomial is of low degree, multiple\n",
    "polynomials are needed to span the entire table. If some care is taken,\n",
    "the set of polynomials so obtained will behave well enough to be used in\n",
    "further calculations without introducing much unwanted noise or\n",
    "discontinuities.\n",
    "\n",
    "![image](Figs/Fig7_5.png)\n",
    "\n",
    "**Figure 7.5** Three fits to data. *Dashed:* Lagrange interpolation\n",
    "using an eight-degree polynomial; *Short dashes:* cubic splines fit ;\n",
    "*Long dashed:* Least-squares parabola fit.\n",
    "\n",
    "The classic interpolation formula was created by Lagrange.He figured out\n",
    "a closed-form expression that directly fits the ($n-1$)-order polynomial\n",
    "(7.23) to $n$ values of the function $g(x)$ evaluated at the points\n",
    "$x_{i}$. The formula for each interval is written as the sum of\n",
    "polynomials: \n",
    "\n",
    "$$\\begin{align}\\tag*{7.23}\n",
    "g(x) &\\simeq  g_{1}\\lambda_{1}(x) + g_{2}\\lambda_{2}(x) + \\cdots +\n",
    "g_{n}\\lambda_{n}(x),\\\\\n",
    "\\lambda_{i}(x) & =  \\prod_{j (\\neq i)=1}^{n}\n",
    "\\frac{x-x_{j}}{x_{i}-x_{j}} = \\frac{x-x_1} { x_i-x_1} \\frac{x-x_2}\n",
    "{ x_i-x_2} \\cdots \\frac{x-x_n} { x_i-x_n}.\\tag*{7.24}\\end{align}$$\n",
    "\n",
    "For three points (7.23) provides a second-degree polynomial, while for\n",
    "eight points it gives a seventh-degree polynomial. For example, assume\n",
    "we are given the points and function values\n",
    "\n",
    "$$\\tag*{7.25}\n",
    "x_{1-4}  = (0 ,1 ,2, 4) \\qquad  g_{1-4}  =(-12,-12,-24,-60).$$\n",
    "\n",
    "With four points, the Lagrange formula determines a third-order\n",
    "polynomial that reproduces each of the tabulated values:\n",
    "\n",
    "$$\\begin{align}\\tag*{7.26}\n",
    "g(x) &=  \\frac{(x-1)(x-2)(x-4)}{(0-1)(0-2)(0-4)}(-12) +\n",
    "\\frac{x(x-2)(x-4)}{(1-0)(1-2)(1-4)}(-12)  \\\\\n",
    "&+ \\frac{x(x-1)(x-4)}{(2-0)(2-1)(2-4)}(-24) +\n",
    "\\frac{x(x-1)(x-2)}{(4-0)(4-1)(4-2)}(-60),  \\\\\n",
    "\\Rightarrow \\quad g(x) &=  x^{3} - 9x^{2} + 8x -12.\\end{align}$$\n",
    "\n",
    "As a check we see that\n",
    "\n",
    "$$\\tag*{7.27}\n",
    "g(4) = 4^{3} -9(4^{2}) +32 -12 = -60, \\qquad g(0.5)= -10.125.$$\n",
    "\n",
    "If the data contain little noise, this polynomial can be used with some\n",
    "confidence within the range of the data, but with risk beyond the range\n",
    "of the data.\n",
    "\n",
    "Notice that Lagrange interpolation makes no restriction that the points\n",
    "$x_i$ be evenly spaced. Usually the Lagrange fit is made to only a small\n",
    "region of the table with a small value of $n$, despite the fact that the\n",
    "formula works perfectly well for fitting a high-degree polynomial to the\n",
    "entire table. The difference between the value of the polynomial\n",
    "evaluated at some $x$ and that of the actual function can be shown to be\n",
    "the *remainder*\n",
    "\n",
    "$$\\tag*{7.28}\n",
    "R_{n} \\simeq \\frac{(x-x_{1})(x-x_{2}) \\cdots (x-x_{n})}{n!}\n",
    "g^{(n)}(\\zeta),$$\n",
    "\n",
    "where $\\zeta$ lies somewhere in the interpolation interval. What\n",
    "significant here is that we see that if significant high derivatives\n",
    "exist in $g(x)$, then it cannot be approximated well by a polynomial.\n",
    "For example, a table of noisy data would have significant high\n",
    "derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7.5.2  Cubic Spline Interpolation (Method)<a id=\"7.5.2\"></a>\n",
    "\n",
    "If you tried to interpolate the resonant cross section with Lagrange\n",
    "interpolation, then you saw that fitting parabolas (three-point\n",
    "interpolation) within a table may avoid the erroneous and possibly\n",
    "catastrophic deviations of a high-order formula. (A two-point\n",
    "interpolation, which connects the points with straight lines, may not\n",
    "lead you far astray, but it is rarely pleasing to the eye or precise.) A\n",
    "sophisticated variation of an $n=4$ interpolation, known as *cubic\n",
    "splines*, often leads to surprisingly eye-pleasing fits. In this\n",
    "approach (Figure 7.5), cubic polynomials are fit to the function in each\n",
    "interval, with the additional constraint that the first and second\n",
    "derivatives of the polynomials be continuous from one interval to the\n",
    "next. This continuity of slope and curvature is what makes the spline\n",
    "fit particularly eye-pleasing. It is analogous to what happens when you\n",
    "use the flexible spline drafting tool (a lead wire within a rubber\n",
    "sheath) from which the method draws its name.\n",
    "\n",
    "The series of cubic polynomials obtained by spline-fitting a table of\n",
    "data can be integrated and differentiated and is guaranteed to have\n",
    "well-behaved derivatives. The existence of meaningful derivatives is an\n",
    "important consideration. As a case in point, if the interpolated\n",
    "function is a potential, you can take the derivative to obtain the\n",
    "force. The complexity of simultaneously matching polynomials and their\n",
    "derivatives over all the interpolation points leads to many simultaneous\n",
    "linear equations to be solved. This makes splines unattractive for hand\n",
    "calculation, yet easy for computers and, not surprisingly, popular in\n",
    "both calculations and computer drawing programs. To illustrate, the\n",
    "smooth solid curve in Figure 7.5 is a spline fit.\n",
    "\n",
    "The basic approximation of splines is the representation of the function\n",
    "$g(x)$ in the subinterval $[x_{i},x_{i+1}]$ with a cubic polynomial:\n",
    "\n",
    "$$\\begin{align}\\tag*{7.29}\n",
    "g(x) &\\simeq   g_{i}(x), \\quad \\mbox{for}\\ \\ x_i\\leq x \\leq\n",
    "x_{i+1},\\\\\n",
    "g_{i}(x) &=  g_{i} +g_i'(x-x_{i})\n",
    "+\\frac{1}{2} g_i''(x-x_{i})^{2}\n",
    "+\\frac{1}{6}g_{i}(x-x_{i})^{3}.\\tag*{7.30}\n",
    "\\end{align}$$ \n",
    "\n",
    "This\n",
    "representation makes it clear that the coefficients in the polynomial\n",
    "equal the values of $g(x)$ and its first, second, and third derivatives\n",
    "at the tabulated points $x_i$. Derivatives beyond the third vanish for a\n",
    "cubic. The computational chore is to determine these derivatives in\n",
    "terms of the $N$ tabulated $g_{i}$ values. The matching of $g_i$ at the\n",
    "*nodes* that connect one interval to the next provides the equations\n",
    "\n",
    "$$\\tag*{7.31}\n",
    "g_{i}(x_{i+1}) = g_{i+1}(x_{i+1}), \\quad i=1, N-1.$$\n",
    "\n",
    "The matching of the first *and* second derivatives at each interval’s\n",
    "boundaries provides the equations\n",
    "\n",
    "$$\\tag*{7.32}\n",
    "g_{i-1}'(x_{i}) = g_{i}'(x_{i}), \\quad g_{i-1}\"(x_{i}) =\n",
    "g_{i}\"(x_{i}).$$\n",
    "\n",
    "The additional equations needed to determine all constants is obtained\n",
    "by matching the third derivatives at adjacent nodes. Values for the\n",
    "third derivatives are found by approximating them in terms of the second\n",
    "derivatives:\n",
    "\n",
    "$$\\tag*{7.33}\n",
    "g_{i}\"' \\simeq \\frac{g_{i+1}\" - g_{i}\"}{x_{i+1} - x_{i}} .$$\n",
    "\n",
    "As discussed in [Chapter 5, *Differentiation &\n",
    "Integration*](CP05.ipynb), a *central-difference approximation* would be\n",
    "more accurate than a forward-difference approximation, yet (7.23) keeps\n",
    "the equations simpler.\n",
    "\n",
    "It is straightforward although complicated to solve for all the\n",
    "parameters in (7.30). We leave that to the references \\[[Thompson(92)](BiblioLinked.html#thompson),\n",
    "[Press et al.(94)](BiblioLinked.html#press)\\]. We can see, however, that matching at the boundaries\n",
    "of the intervals results in only $(N-2)$ linear equations for $N$\n",
    "unknowns. Further input is required. It usually is taken to be the\n",
    "boundary conditions at the endpoints $a =\n",
    "x_{1}$ and $b = x_{N}$, specifically, the second derivatives $g\"(a)$ and\n",
    "$g\"(b)$. There are several ways to determine these second derivatives:\n",
    "\n",
    "<span>Natural spline:</span>\n",
    "\n",
    "-    Set $g\"(a) = g\"(b) = 0$; that is, permit the function to have a\n",
    "    slope at the endpoints but no curvature. This is “natural” because\n",
    "    the derivative vanishes for the flexible spline drafting tool (its\n",
    "    ends being free).\n",
    "\n",
    "<span>Input values for $g'$ at the boundaries:</span>\n",
    "\n",
    "-    The computer uses $g'(a)$ to approximate $g\"(a)$. If you do not\n",
    "    know the first derivatives, you can calculate them numerically from\n",
    "    the table of $g_{i}$ values.\n",
    "\n",
    "<span>Input values for $g\"$ at the boundaries:</span>\n",
    "\n",
    "-    Knowing values is of course better than approximating values, but\n",
    "    it requires the user to input information. If the values of $g\"$ are\n",
    "    not known, they can be approximated by applying a forward-difference\n",
    "    approximation to the tabulated values:\n",
    "\n",
    "$$ g\"(x) \\simeq \\frac{[g(x_{3})-g(x_{2})]/[x_{3}-x_{2}] - [g(x_{2})-g(x_{1})]/[x_{2}-x_{1}]}{[x_{3}-x_{1}]/2}.\\tag*{7.34}$$\n",
    "\n",
    "### Cubic Spline Quadrature (Exploration)\n",
    "\n",
    "A powerful integration scheme is to fit an integrand with splines and\n",
    "then integrate the cubic polynomials analytically. If the integrand\n",
    "$g(x)$ is known only at its tabulated values, then this is about as good\n",
    "an integration scheme as is possible; if you have the ability to\n",
    "calculate the function directly for arbitrary $x$, Gaussian quadrature\n",
    "may be preferable. We know that the spline fit to $g$ in each interval\n",
    "is the cubic (7.30)\n",
    "\n",
    "$$\\tag*{7.35}\n",
    "g(x) \\simeq g_{i} +g_{i}'(x-x_{i}) + \\frac{1}{2}g_{i}\"(x -\n",
    "x_{i})^{2} +\\frac{1}{6}g_{i}\"'(x-x_{i})^{3}.$$\n",
    "\n",
    "It is easy to integrate this to obtain the integral of $g$ for this\n",
    "interval and then to sum over all intervals:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag{7.36}\n",
    "\\int_{x_{i}}^{x_{i+1}} g(x)  dx &\\simeq  \\left. \\left(g_{i}x +\n",
    "\\frac{1}{2}g_{i}' x^{2} + \\frac{1}{6}g_{i}\" x^{3} +\n",
    "\\frac{1}{24}g_{i}\"' x^{4}\\right)\\right|_{x_{i}}^{x_{i+1}} ,\n",
    "\\\\\n",
    "\\int_{x_{j}}^{x_{k}} g(x)   dx &=  \\sum_{i=j}^{k} \\left.\n",
    "\\left(g_{i}x + \\frac{1}{2}g_{i}' x_{i}^{2} + \\frac{1}{6}g_{i}\"\n",
    "x^{3} + \\frac{1}{24}g_{i}\"' x^{4}\n",
    "\\right)\\right|_{x_{i}}^{x_{i+1}}.\\tag{7.37}\\end{align}$$\n",
    "\n",
    "Making the intervals smaller does not necessarily increase precision, as\n",
    "subtractive cancellations in (7.37) may get large.\n",
    "\n",
    "[![image](Figs/Javaapplet5.png)Spline\n",
    "Interp](http://science.oregonstate.edu/~rubin/Books/CPbook/eBook/Applets/index.html)\n",
    "\n",
    "**Spline Fit of Cross Section (Implementation):** Fitting a series of\n",
    "cubics to data is a little complicated to program yourself, so we\n",
    "recommend using a library routine. While we have found quite a few\n",
    "Java-based spline applications available on the internet, none seemed\n",
    "appropriate for interpreting a simple set of numbers. That being the\n",
    "case, we have adapted the `splint.c` and the `spline.c` functions from\n",
    "\\[Press et al.(94)\\] to produce the `SplineInterp.py` program shown in\n",
    "Listing 7.3 (there is also an applet). Your **problem** for this section\n",
    "is to carry out the assessment in § 7.5.1 using cubic spline\n",
    "interpolation rather than Lagrange interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SplineInterp.py, Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SplineInterp.py Cubic Splie Interpolation\n",
    "\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = array([0., 0.12, 0.25, 0.37, 0.5, 0.62, 0.75, 0.87, 0.99])                # input\n",
    "y = array([10.6, 16.0, 45.0, 83.5, 52.8, 19.9, 10.8, 8.25, 4.7])\n",
    "n = 9\n",
    "npo = 15\n",
    "Nfit=100\n",
    "y2 = np.zeros( (n), float)\n",
    "u = np.zeros( (n), float)\n",
    "xout=np.zeros((Nfit+2),float)\n",
    "yout=np.zeros((Nfit+2),float)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(x,y,'ro')\n",
    "yp1 = (y[1]-y[0])/(x[1]-x[0])-(y[2]-y[1])/(x[2]-x[1])+(y[2]-y[0])/(x[2]-x[0])\n",
    "ypn = (y[n-1] - y[n-2])/(x[n-1] - x[n-2]) - (y[n-2] - y[n-3])/(x[n-2] - \n",
    "      x[n-3]) + (y[n-1] - y[n-3])/(x[n - 1] - x[n - 3])\n",
    "for i in range(1, n - 1):                    # Decomposition loop\n",
    "        sig = (x[i] - x[i - 1])/(x[i + 1] - x[i - 1]) \n",
    "        p = sig*y2[i - 1] + 2. \n",
    "        y2[i] = (sig - 1.)/p \n",
    "        u[i] = (y[i+1] - y[i])/(x[i+1] - x[i]) - (y[i] - y[i-1])/(x[i] - x[i-1]) \n",
    "        u[i] = (6.*u[i]/(x[i + 1] - x[i - 1]) - sig*u[i - 1])/p\n",
    "        qn=0.5\n",
    "        un=(3/(x[n-1]-x[n-2]))*(ypn-(y[n-1]-y[n-2])/(x[n-1]-x[n-2]))\n",
    "y2[n - 1] = (un - qn*u[n - 2])/(qn*y2[n - 2] + 1.)\n",
    "for k in range(n - 2, 1,  - 1):\n",
    "        y2[k] = y2[k]*y2[k + 1] + u[k]\n",
    "for i in range(1, Nfit + 2):                     # initialization ends, begin fit\n",
    "        xout[i] = x[0] + (x[n - 1] - x[0])*(i - 1)/(Nfit) \n",
    "        klo = 0;    khi = n - 1                  # Bisection algor\n",
    "        while (khi - klo >1):\n",
    "            k = (khi + klo) >> 1\n",
    "            if (x[k] > xout[i]): \n",
    "                khi  = k\n",
    "            else: \n",
    "                klo = k\n",
    "        h = x[khi] - x[klo] \n",
    "        if (x[k] > xout[i]):  \n",
    "            khi = k\n",
    "        else: \n",
    "            klo = k \n",
    "        h = x[khi] - x[klo]\n",
    "        a = (x[khi] - xout[i])/h \n",
    "        b = (xout[i] - x[klo])/h \n",
    "        yy = (a*y[klo]+b*y[khi] + ((a**3-a)*y2[klo] + \n",
    "                 (b**3-b)*y2[khi])*(h*h)/6.)\n",
    "        yout[i]=yy\n",
    "ax.plot(xout,yout,'b',lw=2)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1  Lagrange Implementation, Assessment<a id=\"7.5.1\"></a>\n",
    "\n",
    "Consider the experimental neutron scattering data in Table 7.1. The\n",
    "expected theoretical functional form that describes these data is\n",
    "(7.21), and our empirical fits to these data are shown in Figure 7.5.\n",
    "\n",
    "1.  Write a subroutine to perform an $n$-point Lagrange interpolation\n",
    "    using (7.23). Treat $n$ as an arbitrary input parameter. (You may\n",
    "    also do this exercise with the spline fits discussed in §7.5.2.)\n",
    "\n",
    "2.  Use the Lagrange interpolation formula to fit the entire\n",
    "    experimental spectrum with one polynomial. (This means that you must\n",
    "    fit all nine data points with an eight-degree polynomial.) Then use\n",
    "    this fit to plot the cross section in steps of 5 MeV.\n",
    "\n",
    "3.  Use your graph to deduce the resonance energy $E_{r}$ (your\n",
    "    peak position) and $\\Gamma$ (the full width at half-maximum).\n",
    "    Compare your results with those predicted by a theorist friend,\n",
    "    $(E_{r}, \\Gamma) = (78, 55) \\mbox{MeV}$.\n",
    "\n",
    "4.  A more realistic use of Lagrange interpolation is for local\n",
    "    interpolation with a small number of points, such as three.\n",
    "    Interpolate the preceding cross-sectional data in 5-MeV steps using\n",
    "    three-point Lagrange interpolation for each interval. (Note that the\n",
    "    end intervals may be special cases.)\n",
    "\n",
    "5.  We deliberately have not discussed *extrapolation* of data because\n",
    "    it can lead to serious *systematic* errors; the answer you get may\n",
    "    well depend more on the function you assume than on the data\n",
    "    you input. Add some adventure to your life and use the programs you\n",
    "    have written to extrapolate to values outside Table 7.1. Compare\n",
    "    your results to the theoretical Breit-Wigner shape (7.21).\n",
    "\n",
    "This example shows how easy it is to go wrong with a\n",
    "high-degree-polynomial fit to data with errors. Although the polynomial\n",
    "is guaranteed to pass through all the data points, the representation of\n",
    "the function away from these points can be quite unrealistic. Using a\n",
    "low-order interpolation formula, say, $n=2$ or $3$, in each interval\n",
    "usually eliminates the wild oscillations, but may not have any\n",
    "theoretical justification. If these local fits are matched together, as\n",
    "we discuss in the next section, a rather continuous curve results.\n",
    "Nonetheless, you must recall that if the data contain errors, a curve\n",
    "that actually passes through them may lead you astray. We discuss how to\n",
    "do this properly with least-square fitting in § 7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Figs/Fig7_6.png)\n",
    "\n",
    " **Figure 7.6** A reproduction of the experimental measurement of \\[[Stetz et al.(73)](BiblioLinked.html#stetz)\\] giving the number of\n",
    "decays of a *π* meson as a function of time since its creation.\n",
    "Measurements were made during time intervals (box sizes) of 10-ns width.\n",
    "The dashed curve is the result of a linear least-square fit to the\n",
    "log*N*(*t*).\n",
    "\n",
    "## 7.6  Problem 4: Fitting Exponential Decay<a id=\"7.6\"></a>\n",
    "\n",
    "**Problem:** Figure 7.6 presents actual experimental data on the number\n",
    "of decays $\\Delta N$ of the $\\pi$ meson as a function of time\n",
    "\\[[Stetz et al.(73)](BiblioLinked.html#stetz)\\]. Notice that the time has been “binned” into $\\Delta\n",
    "t = 10\\mbox{-ns}$ intervals and that the smooth curve is the theoretical\n",
    "exponential decay expected for very large numbers of pions (which there\n",
    "is not). Your **problem** is to deduce the lifetime $\\tau$ of the\n",
    "$\\pi$ mes on from these data (the tabulated lifetime of the pion is\n",
    "$2.6 \\times 10^{-8} \\mbox{s}$).\n",
    "\n",
    "**Theory:** Assume that we start with $N_{0}$ particles at time $t=0$\n",
    "that can decay to other particles.\\[*Note:* Spontaneous decay is\n",
    "discussed further and simulated in §4.5.\\] If we wait a short time\n",
    "$\\Delta t$, then a small number $\\Delta N$ of the particles will decay\n",
    "*spontaneously*, that is, with no external influences. This decay is a\n",
    "stochastic process, which means that there is an element of chance\n",
    "involved in just when a decay will occur, and so no two experiments are\n",
    "expected to give exactly the same results. The basic law of nature for\n",
    "spontaneous decay is that the number of decays $\\Delta N$ in a time\n",
    "interval $\\Delta t$ is proportional to the number of particles $N(t)$\n",
    "present at that time and to the time interval:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.38}\n",
    "\\Delta  N(t) = - \\frac{1} { \\tau} N(t) \\Delta t \\quad \\Rightarrow \\quad\n",
    "\\frac{\\Delta N(t)}{\\Delta t}= -\\lambda N(t).\\end{align}$$\n",
    "\n",
    "[**Listing 7.3  SplineInteract.py**](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/SplineInteract.py) performs a cubic spline fit to data\n",
    "and permits interactive control. The arrays x\\[ \\] and y\\[ \\] are the\n",
    "data to fit, and the values of the fit at Nfit points are\n",
    "output.\n",
    "\n",
    "Here $\\tau = 1/\\lambda$ is the *lifetime* of the particle, with\n",
    "$\\lambda$ the rate parameter. The actual decay *rate* is given by the\n",
    "second equation in (7.38). If the number of decays $\\Delta N$ is very\n",
    "small compared to the number of particles $N$, and if we look at\n",
    "vanishingly small time intervals, then the difference equation (7.38)\n",
    "becomes the differential equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.39}\n",
    "\\frac{dN(t)}{dt} \\simeq -\\lambda N(t) = \\frac{1}{\\tau}N(t).\\end{align}$$\n",
    "\n",
    "This differential equation has an exponential solution for the number as\n",
    "well as for the decay rate:\n",
    "\n",
    "$$\\tag*{7.40}\n",
    "N(t) = N_{0} e^{-t/\\tau}, \\quad  \\frac{dN(t)}{dt} = - \\frac{N_{0}}\n",
    "{ \\tau} e^{-t/\\tau} =\\frac{dN(0)}{dt}\n",
    "e^{-t/\\tau}.$$\n",
    "\n",
    "Equation (7.40) is the theoretical formula we wish to “fit” to the data\n",
    "in Figure 7.6. The output of such a fit is a “best value” for the\n",
    "lifetime $\\tau$.\n",
    "\n",
    "![image](Figs/Fig7_6.png)\n",
    "\n",
    "**Figure 7.6** A reproduction of the experimental measurement of \\[Stetz\n",
    "et al.(73)\\] giving the number of decays of a $\\pi$ meson as a function\n",
    "of time since its creation. Measurements were made during time intervals\n",
    "(box sizes) of 10-ns width. The dashed curve is the result of a linear\n",
    "least-square fit to the $\\log N(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7  Least-Squares Fitting (Theory)<a id=\"7.7\"></a>\n",
    "\n",
    "Books have been written and careers have been spent discussing what is\n",
    "meant by a “good fit” to experimental data. We cannot do justice to the\n",
    "subject here and refer the reader to \\[[Bevington & Robinson(02)](BiblioLinked.html#bev), [Press\n",
    "et al.(94)](BiblioLinked.html#press), [Thompson(92)](BiblioLinked.html#thompson)\\]. However, we will emphasize three points:\n",
    "\n",
    "1.  If the data being fit contain errors, then the “best fit” in a\n",
    "    statistical sense should not pass through all the data points.\n",
    "\n",
    "2.  If the theory is not an appropriate one for the data (e.g., the\n",
    "    parabola in Figure 7.5), then its best fit to the data may not be a\n",
    "    good fit at all. This is good, for this is how we know that the\n",
    "    theory is not right.\n",
    "\n",
    "3.  Only for the simplest case of a linear least-squares fit can we\n",
    "    write down a closed-form solution to evaluate and obtain the fit.\n",
    "    More realistic problems are usually solved by *trial-and-error*\n",
    "    search procedures, sometimes using sophisticated\n",
    "    subroutine libraries. However, in §7.8.2 we show how to conduct such\n",
    "    a nonlinear search using familiar tools.\n",
    "\n",
    "Imagine that you have measured *N*<sub>*D*</sub> data values of the\n",
    "independent variable *y* as a function of the dependent variable *x*:\n",
    "\n",
    "$$\\tag*{7.41}\n",
    " (x_{i}, y_{i} \\pm \\sigma_{i}), \\quad i=1,N_{D},$$\n",
    "\n",
    "where ±*σ*<sub>*i*</sub> is the experimental uncertainty in the *i*th value of\n",
    "*y*. (For simplicity we assume that all the errors *σ*<sub>*i*</sub> occur in\n",
    "the dependent variable, although this is hardly ever true \\[[Thompson(92)](BiblioLinked.html#thompson)\\]). For\n",
    "our problem, *y* is the number of decays as a function of time, and\n",
    "*x*<sub>*i*</sub> are the times. Our goal is to determine how well a\n",
    "mathematical function *y* = *g*(*x*) (also called a *theory* or a *model*) can\n",
    "describe these data. Additionally, if the theory contains some parameters or\n",
    "constants, our goal can be viewed as determining the best values for these\n",
    "parameters. We assume that the theory function *g*(*x*) contains, in addition\n",
    "to the functional dependence on *x*, an additional dependence upon\n",
    "*M*<sub>*P*</sub> parameters\n",
    "{*a*<sub>1</sub>, *a*<sub>2</sub>, …, *a*<sub>*M*<sub>*P*</sub></sub>}.\n",
    "Notice that the parameters {*a*<sub>*m*</sub>} are not variables, in the sense\n",
    "of numbers read from a meter, but rather are parts of the theoretical model,\n",
    "such as the size of a box, the mass of a particle, or the depth of a potential well.\n",
    "For the exponential decay function (7.40), the parameters are the lifetime *τ*\n",
    "and the initial decay rate *dN*(0)/*dt*. We include the parameters as\n",
    "\n",
    "$$\\tag*{7.42} g(x) = g(x; \\{a_{1},a_{2}, \\ldots, a_{M_{P}}\\}) = g(x;\\{a_{m}\\}) ,$$\n",
    "\n",
    "where the *a*<sub>*i*</sub>’s are parameters and *x* the independent\n",
    "variable. We use the chi-square (*χ*<sup>2</sup>) measure \\[[Bevington &\n",
    "Robinson(02)](BiblioLinked.html#bev)\\] as a gauge of how well a theoretical function *g*\n",
    "reproduces data:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.43.xml)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.43}\n",
    " \\chi^{2}  =  \\sum_{i=1}^{N_{D}}\n",
    "\\left(\\frac{y_{i} - g(x_{i};\\{a_{m}\\})} {\\sigma_{i}}\\right)^{2},\\end{align}$$\n",
    "\n",
    "where the sum is over the *N*<sub>*D*</sub> experimental points\n",
    "(*x*<sub>*i*</sub>, *y*<sub>*i*</sub> ± *σ*<sub>*i*</sub>). The\n",
    "definition (7.43) is such that smaller values of *χ*<sup>2</sup> are\n",
    "better fits, with *χ*<sup>2</sup> = 0 occurring if the theoretical curve\n",
    "went through the center of every data point. Notice also that the\n",
    "1/*σ*<sub>*i*</sub><sup>2</sup> weighting means that measurements with\n",
    "larger errors\\[*Note:* If you are not given the errors, you can guess\n",
    "them on the basis of the apparent deviation of the data from a smooth\n",
    "curve, or you can weigh all points equally by setting\n",
    "*σ*<sub>*i*</sub> ≡ 1 and continue with the fitting.\\] contribute less\n",
    "to *χ*<sup>2</sup>.\n",
    "\n",
    "*Least-squares fitting* refers to adjusting the parameters in the theory\n",
    "until a minimum in *χ*<sup>2</sup> is found, that is, finding a curve\n",
    "that produces the least value for the summed squares of the deviations\n",
    "of the data from the function *g*(*x*). In general, this is the best fit\n",
    "possible and the best way to determine the parameters in a theory. The\n",
    "*M*<sub>*P*</sub> parameters\n",
    "{*a*<sub>*m*</sub>, *m* = 1, *M*<sub>*P*</sub>} that make\n",
    "*χ*<sup>2</sup> an extremum are found by solving the *M*<sub>*P*</sub>\n",
    "equations:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.44.xml)\n",
    "\n",
    "$$\\tag*{7.44}\n",
    "\\frac{\\partial\\chi^{2}}{\\partial a_{m}} = 0, \\quad \\Rightarrow\\ \\;\n",
    "\\sum_{i=1}^{N_{D}}  \\frac{[y_{i}-g(x_{i})]}{\\sigma_{i}^{2}}\n",
    "\\frac{\\partial g(x_i)} {\\partial a_{m}} = 0, \\enspace (m=1,M_{P}).$$\n",
    "\n",
    "Often, the function *g*(*x*; {*a*<sub>*m*</sub>}) has a sufficiently\n",
    "complicated dependence on the *a*<sub>*m*</sub> values for (7.44) to\n",
    "produce *M*<sub>*P*</sub> simultaneous nonlinear equations in the\n",
    "*a*<sub>*m*</sub> values. In these cases, solutions are found by a\n",
    "trial-and-error search through the *M*<sub>*P*</sub>-dimensional\n",
    "parameter space, as we do in § 7.8.2. To be safe, when such a search is\n",
    "completed, you need to check that the minimum *χ*<sup>2</sup> you found\n",
    "is *global* and not *local*. One way to do that is to repeat the search\n",
    "for a whole grid of starting values, and if different minima are found,\n",
    "to pick the one with the lowest *χ*<sup>2</sup>.\n",
    "\n",
    "![image](Figs/Fig7_7.png)\n",
    "\n",
    " **Figure 7.7** A linear least-squares best fit\n",
    "of a straight line to data. The deviation of theory from experiment is\n",
    "greater than would be expected from statistics, which means that a\n",
    "straight line is not a good theory to describe these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1  Least-Squares Fitting: Theory and Implementation<a id=\"7.7.1\"></a>\n",
    "\n",
    "When the deviations from theory are as a result of random errors and\n",
    "when these errors are described by a Gaussian distribution, there are\n",
    "some useful rules of thumb to remember \\[[Bevington & Robinson(02)](BiblioLinked.html#bev)\\]. You\n",
    "know that your fit is good if the value of *χ*<sup>2</sup> calculated\n",
    "via the definition (7.43) is approximately equal to the number of\n",
    "degrees of freedom\n",
    "*χ*<sup>2</sup> ≃ *N*<sub>*D*</sub> − *M*<sub>*P*</sub>, where\n",
    "*N*<sub>*D*</sub> is the number of data points and *M*<sub>*P*</sub> is\n",
    "the number of parameters in the theoretical function. If your\n",
    "*χ*<sup>2</sup> is much less than *N*<sub>*D*</sub> − *M*<sub>*P*</sub>,\n",
    "it doesn’t mean that you have a “great” theory or a really precise\n",
    "measurement; instead, you probably have too many parameters or have\n",
    "assigned errors (*σ*<sub>*i*</sub> values) that are too large. In fact,\n",
    "too small a *χ*<sup>2</sup> may indicate that you are fitting the random\n",
    "scatter in the data rather than missing approximately one-third of the\n",
    "error bars, as expected if the errors are random. If your\n",
    "*χ*<sup>2</sup> is significantly greater than\n",
    "*N*<sub>*D*</sub> − *M*<sub>*P*</sub>, the theory may not be good, you\n",
    "may have significantly underestimated your errors, or you may have\n",
    "errors that are not random.\n",
    "\n",
    "The *M*<sub>*P*</sub> simultaneous equations (7.44) can be simplified\n",
    "considerably if the functions *g*(*x*; {*a*<sub>*m*</sub>}) depend\n",
    "*linearly* on the parameter values *a*<sub>*i*</sub>, e.g.,\n",
    "\n",
    "$$\\tag*{7.45} g\\left(x; \\{a_{1}, a_{2}\\}\\right) = a_{1} + a_{2}x.$$\n",
    "\n",
    "In this case (also known as *linear regression or straight-line fit*),\n",
    "as shown in Figure 7.7, there are *M*<sub>*P*</sub> = 2 parameters, the\n",
    "slope *a*<sub>2</sub>, and the *y* intercept *a*<sub>1</sub>. Notice\n",
    "that while there are only two parameters to determine, there still may\n",
    "be an arbitrary number *N*<sub>*D*</sub> of data points to fit.\n",
    "Remember, a unique solution is not possible unless the number of data\n",
    "points is equal to or greater than the number of parameters. For this\n",
    "linear case, there are just two derivatives,\n",
    "\n",
    "$$\\tag*{7.46}\n",
    "\\frac{\\partial g(x_i)}{\\partial a_1}=1, \\quad\\frac{\\partial\n",
    "g(x_i)}{\\partial a_2} = x_i,$$\n",
    "\n",
    "and after substitution, the *χ*<sup>2</sup> minimization equations\n",
    "(7.44) can be solved \\[[Press et al.(94)](BiblioLinked.html#press)\\]:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.47.xml)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.47}\n",
    "a_{1} &= \\frac{S_{xx}S_{y} - S_{x}S_{xy}}{\\Delta}, \\ &&\\ & a_{2} &=\n",
    "\\frac{SS_{xy} - S_{x}S_{y}}{\\Delta}, \\\\\n",
    "S &=\\sum_{i=1}^{N_{D}} \\frac{1}{\\sigma_{i}^{2}} , \\ \\ &S_{x} &nn=\n",
    "\\sum_{i=1}^{N_{D}} \\frac{x_{i}}{\\sigma_{i}^{2}},\\ \\  &n S_{y} &nn=\n",
    "\\sum_{i=1}^{N_{D}}\n",
    "\\frac{y_{i}}{\\sigma_{i}^{2}},\\tag*{7.48}\\\\\n",
    "S_{xx} &= \\sum_{i=1}^{N_{D}} \\frac{x_{i}^{2}}{\\sigma_{i}^{2}},\n",
    "\\ \\ &S_{xy} &n= \\sum_{i=1}^{N_{D}}\n",
    "\\frac{x_{i}y_{i}}{\\sigma_{i}^{2}},\\ \\  &\\Delta &= SS_{xx} -\n",
    "S_{x}^{2} .\\tag*{7.49}\\end{align}$$\n",
    "\n",
    "![image](Figs/Fig7_8.png)\n",
    "\n",
    " **Figure 7.8** A linear least-squares best fit\n",
    "of a parabola to data. Here we see that the fit misses approximately\n",
    "one-third of the points, as expected from the statistics for a good fit.\n",
    "\n",
    "Statistics also gives you an expression for the *variance* or\n",
    "uncertainty in the deduced parameters:\n",
    "\n",
    "$$\\tag*{7.50}\n",
    "\\sigma_{a_{1}}^{2} = \\frac{S_{xx}}{\\Delta}, \\quad\n",
    "\\sigma_{a_{2}}^{2} = \\frac{S}{\\Delta}.$$\n",
    "\n",
    "This is a measure of the uncertainties in the values of the fitted\n",
    "parameters arising from the uncertainties *σ*<sub>*i*</sub> in the\n",
    "measured *y*<sub>*i*</sub> values. A measure of the dependence of the\n",
    "parameters on each other is given by the *correlation coefficient*:\n",
    "\n",
    "$$\\tag*{7.51}\n",
    "\\rho(a_{1},a_{2}) =\n",
    "\\frac{\\mbox{cov}(a_{1},a_{2})}{\\sigma_{a_{1}}\\sigma_{a_{2}}},\\quad\n",
    "\\mbox{cov}(a_{1},a_{2}) = \\frac{-S_{x}}{\\Delta}.$$\n",
    "\n",
    "Here *cov*(*a*<sub>1</sub>, *a*<sub>2</sub>) is the *covariance* of\n",
    "*a*<sub>1</sub> and *a*<sub>2</sub> and vanishes if *a*<sub>1</sub> and\n",
    "*a*<sub>2</sub> are independent. The correlation coefficient\n",
    "*ρ*(*a*<sub>1</sub>, *a*<sub>2</sub>) lies in the range −1 ≤ *ρ* ≤ 1, with a\n",
    "positive *ρ* indicating that the errors in *a*<sub>1</sub> and\n",
    "*a*<sub>2</sub> are likely to have the same sign, and a negative *ρ* indicating\n",
    "opposite signs.\n",
    "\n",
    "The preceding analytic solutions for the parameters are of the form found in\n",
    "statistics books but are not optimal for numerical calculations because\n",
    "subtractive cancellation can make the answers unstable. As discussed in\n",
    "[Chapter 17, *Errors & Uncertainties in Computations*](CP17.ipynb), a\n",
    "rearrangement of the equations can decrease this type of error. For example,\n",
    "\\[Thompson(92)\\] gives improved expressions that measure the data relative to\n",
    "their averages:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/7.52.xml)\n",
    "\n",
    "$$\\begin{align}\n",
    " a_{1} & =  \\overline{y} - a_{2}\\overline{x} , \\quad\n",
    "a_{2} = \\frac{S_{xy}}{S_{xx}}, \\quad \\overline{x} =\n",
    "\\frac{1}{N}\\sum_{i=1}^{N_{d}}x_{i}, \\quad \\overline{y} =\n",
    "\\frac{1}{N}\\sum_{i=1}^{N_{d}}y_{i}  \\\\\n",
    "S_{xy} & = \\sum_{i=1}^{N_{d}} \\frac{(x_{i}-\\overline{x})(y_{i}\n",
    "-\\overline{y})}{\\sigma_{i}^{2}},\\quad S_{xx} = \\sum_{i=1}^{N_{d}}\n",
    " \\frac{(x_{i}-\\overline{x})^{2}}{\\sigma_{i}^{2}}         .\\tag*{7.52}\\end{align}$$\n",
    "\n",
    "In `Fit.py` in Listing 7.4 we give a program that fits a parabola to\n",
    "some data. You can use it as a model for fitting a line to data,\n",
    "although you can use our closed-form expressions for a straight-line\n",
    "fit. In `Fit.py` on the instructor’s site we give a program for fitting\n",
    "to the decay data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit.py, Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit.py,     Linear least square fit; e.g. of matrix computation arrays\n",
    "\n",
    "import pylab as p\n",
    "#from visual import *\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import solve\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "t = arange(1.0, 2.0, 0.1)                             # x range curve\n",
    "x = array([1., 1.05, 1.15, 1.32,  1.51, 1.68, 1.92])  # Given x values\n",
    "y = array([0.52, 0.73, 1.08, 1.44, 1.39, 1.46, 1.58]) # Given y values\n",
    "p.plot(x, y, 'bo' )                              # Plot data in blue\n",
    "sig = array([0.1, 0.1, 0.2, 0.3, 0.2, 0.1, 0.1])      # error bar lenghts\n",
    "p.errorbar(x,y,sig)                              # Plot error bars\n",
    "p.title('Linear least square fit')               # Plot figure\n",
    "p.xlabel( 'x' )                                  # Label axes\n",
    "p.ylabel( 'y' )\n",
    "p.grid(True)                                     # plot grid\n",
    "Nd = 7\n",
    "A = zeros( (3,3), float )                       # Initialize\n",
    "bvec = zeros( (3,1), float )\n",
    "ss= sx = sxx = sy = sxxx = sxxxx = sxy = sxy = sxxy = 0.\n",
    "\n",
    "for i in range(0, Nd):                                      \n",
    "        sig2 = sig[i] * sig[i]\n",
    "        ss  += 1. / sig2\n",
    "        sx   += x[i]/sig2\n",
    "        sy    += y[i]/sig2\n",
    "        rhl  = x[i] * x[i]\n",
    "        sxx  += rhl/sig2\n",
    "        sxxy  += rhl * y[i]/sig2\n",
    "        sxy += x[i]*y[i]/sig2\n",
    "        sxxx += rhl*x[i]/sig2\n",
    "        sxxxx += rhl * rhl/sig2\n",
    "        \n",
    "A    = array([ [ss,sx,sxx], [sx,sxx,sxxx], [sxx,sxxx,sxxxx] ])\n",
    "bvec = array([sy, sxy, sxxy])\n",
    "\n",
    "xvec = multiply(inv(A), bvec)                       # Invert matrix\n",
    "Itest = multiply(A, inv(A))                         # Matrix multiply\n",
    "print('\\n x vector via inverse')                                       \n",
    "print(xvec, '\\n')\n",
    "print('A*inverse(A)')\n",
    "print(Itest, '\\n')\n",
    "\n",
    "xvec = solve(A, bvec)                     # Solution via elimination\n",
    "print('x Matrix via direct') \n",
    "print(xvec, 'end= ') \n",
    "print('FitParabola Final Results\\n') \n",
    "print('y(x) = a0 + a1 x + a2 x^2')                                    # The desired fit\n",
    "print('a0 = ', x[0])                  \n",
    "print('a1 = ', x[1])\n",
    "print('a2 = ', x[2], '\\n')\n",
    "print(' i   xi     yi    yfit   ')\n",
    "for i in range(0, Nd):\n",
    "    s = xvec[0] + xvec[1]*x[i] + xvec[2]*x[i]*x[i]\n",
    "    print(\" %d %5.3f  %5.3f  %8.7f \\n\"  %(i, x[i], y[i], s))\n",
    "# red line is the fit, red dots the fits at y[i]\n",
    "curve  = xvec[0] + xvec[1]*t + xvec[2]*t**2\n",
    "points = xvec[0] + xvec[1]*x + xvec[2]*x**2\n",
    "p.plot(t, curve,'r', x, points, 'ro')\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8  Exercises: Fitting Exponential Decay, Heat Flow & Hubble’s Law<a id=\"7.8\"></a>\n",
    "\n",
    "**Table 7.2** Temperature versus distance as measured along a\n",
    "    metal rod.\n",
    "\n",
    "|*x*<sub>*i*</sub> (cm) | 1.0 | 2.0 | 3.0 | 4.0 | 5.0 | 6.0 | 7.0 |8.0 | 9.0 |\n",
    "|- - -|- - - |- - -|- - - |- - -|- - - |- - -|- - - |- - -|- - - |\n",
    "| *T*<sub>*i*</sub> (C) | 14.6 | 18.5 | 36.6 | 30.8 | 59.2 | 60.1 |62.2 | 79.4 | 99.9|\n",
    "\n",
    "**Table 7.3** Distance versus radial velocity for 24 extragalactic nebulae.\n",
    "\n",
    "| *Object* | *r* (Megaparsecs) | *v* (km/s) | *Object*|*r* (Megaparsecs) | *v* (km/s)|\n",
    "|- - -:|:- - -:|- - -:|- - -:|:- - -:|- - -:|\n",
    "| |0.032 |170 | 3627|0.9 |650 |\n",
    "| |0.034 |290 | 4826|0.9 |150 |\n",
    "| 6822|0.214 |-130 | 5236|0.9 | 500 |\n",
    "| 598|0.263 |-70 | 1068|1.0 |920 |\n",
    "| 221|0.275 |-185 | 5055|1.1 |450|\n",
    "| 224|0.275 |-220 | 7331|1.1 |500 |\n",
    "| 5457|0.45 |200 | 4258|1.4 |500 |\n",
    "|4736|0.5 |290 | 4141|1.7 |960 |\n",
    "| 5194|0.5| 270 | 4382|2.0 |500 |\n",
    "|4449| 0.63 |200 | 4472|2.0 |850 |\n",
    "| 4214|0.8| 300 |4486| 2.0|800 |\n",
    "|3031|0.9 |-30 | 4649|2.0 |1090|\n",
    "    \n",
    "1.  Fit the exponential decay law (7.40) to the data in Figure 7.6. This\n",
    "    means finding values for *τ* and *ΔN*(0)/*Δt* that provide a\n",
    "    best fit to the data, and then judging how good the fit is.\n",
    "\n",
    "    1.  Construct a table of approximate values for\n",
    "        (*ΔN*/*Δt*<sub>*i*</sub>, *t*<sub>*i*</sub>), for\n",
    "        *i* = 1, *N*<sub>*D*</sub> as read from Figure 7.6. Because time\n",
    "        was measured in bins, *t*<sub>*i*</sub> should correspond to the\n",
    "        middle of a bin.\n",
    "\n",
    "    2.  Add an estimate of the error *σ*<sub>*i*</sub> to obtain a table\n",
    "        of the form\n",
    "        (*ΔN*/*Δt*<sub>*i*</sub> ± *σ*<sub>*i*</sub>, *t*<sub>*i*</sub>).\n",
    "        You can estimate the errors by eye, say, by estimating how much\n",
    "        the histogram values appear to fluctuate about a smooth curve,\n",
    "        or you can take $\\sigma_i \\simeq \\sqrt{\\mbox{events}}$.\n",
    "        (This last approximation is reasonable for large numbers, which\n",
    "        this is not.)\n",
    "\n",
    "    3.  In the limit of very large numbers, we would expect a plot of\n",
    "        ln|*dN*/*dt*| *versus* *t* to be a straight line:\n",
    "\n",
    "        $$\\tag*{7.53}\n",
    "        \\ln \\left|\\frac{\\Delta N(t)}{\\Delta t}\\right| \\simeq \\ln\n",
    "        \\left|\\frac{\\Delta N_{0}}{\\Delta t}\\right| - \\frac{1}{\\tau}\\Delta\n",
    "        t.$$\n",
    "\n",
    "        This means that if we treat ln|*ΔN*(*t*)/*Δt*| as the\n",
    "        dependent variable and time *Δt* as the independent variable,\n",
    "        we can use our linear fit results. Plot ln|*ΔN*/*Δt*|\n",
    "        *versus* *Δt*.\n",
    "\n",
    "    4.  Make a least-squares fit of a straight line to your data and use\n",
    "        it to determine the lifetime *τ* of the *π* meson. Compare your\n",
    "        deduction to the tabulated lifetime of 2.6 × 10<sup>−8</sup> s\n",
    "        and comment on the difference.\n",
    "\n",
    "    5.  Plot your best fit on the same graph as the data and comment on\n",
    "        the agreement.\n",
    "\n",
    "    6.  Deduce the goodness of fit of your straight line and the\n",
    "        approximate error in your deduced lifetime. Do these agree with\n",
    "        what your “eye” tells you?\n",
    "\n",
    "    7.  Now that you have a fit, look at the data again and estimate\n",
    "        what a better value for the errors in the ordinates might be.\n",
    "\n",
    "2.  Table 7.2 gives the temperature *T* along a metal rod whose ends are\n",
    "    kept at a fixed constant temperature. The temperature is a function\n",
    "    of the distance *x* along the rod.\n",
    "    1.  Plot the data in Table 7.3 to verify the appropriateness of a\n",
    "        linear relation\n",
    "\n",
    "        $$\\tag*{7.54}\n",
    "        T(x) \\simeq a + bx.$$\n",
    "\n",
    "    2.  Because you are not given the errors for each measurement,\n",
    "        assume that the least significant figure has been rounded off\n",
    "        and so *σ* ≥ 0.05.\n",
    "\n",
    "    3.  Use that to compute a least-squares straight-line fit to\n",
    "        these data.\n",
    "\n",
    "    4.  Plot your best *a* + *bx* on the curve with the data.\n",
    "\n",
    "    5.  After fitting the data, compute the variance and compare it to\n",
    "        the deviation of your fit from the data. Verify that about\n",
    "        one-third of the points miss the *σ* error band (that’s what is\n",
    "        expected for a normal distribution of errors).\n",
    "\n",
    "    6.  Use your computed variance to determine the *χ*<sup>2</sup> of\n",
    "        the fit. Comment on the value obtained.\n",
    "\n",
    "    7.  Determine the variances *σ*<sub>*a*</sub> and *σ*<sub>*b*</sub>\n",
    "        and check whether it makes sense to use them as the errors in\n",
    "        the deduced values for *a* and *b*.\n",
    "\n",
    "3.  In 1929 Edwin Hubble examined the data relating the radial velocity\n",
    "    *v* of 24 extra galactic nebulae to their distance *r* from our\n",
    "    galaxy \\[[Hubble(29)](BiblioLinked.html#hub)\\]. Although there was considerable scatter in\n",
    "    the data, he fit them with a straight line:\n",
    "\n",
    "    $$\\tag*{7.55}\n",
    "    v = H r,$$\n",
    "\n",
    "    where *H* is now called Hubble constant. Table 7.3 contains the\n",
    "    distances and velocities used by Hubble.\n",
    "\n",
    "       1.  Plot the data to verify the appropriateness of a linear relation\n",
    "\n",
    "        $$\\tag*{7.56}\n",
    "        v(r) \\simeq a + Hr.$$\n",
    "\n",
    "    2.  Because you are not given the errors for each measurement, you\n",
    "        may assume that the least significant figure has been rounded\n",
    "        off and so *σ* ≥ 1. Or, you may assume that astronomical\n",
    "        measurements are hard to make and that there are at least 10%\n",
    "        errors in the data.\n",
    "\n",
    "    3.  Compute a least-squares straight-line fit to these data.\n",
    "\n",
    "    4.  Plot your best *a* + *Hr* on the curve with the data.\n",
    "\n",
    "    5.  After fitting the data, compute the variance and compare it to\n",
    "        the deviation of your fit from the data. Verify that about\n",
    "        one-third of the points miss the *σ* error band (that’s what is\n",
    "        expected for a normal distribution of errors).\n",
    "\n",
    "    6.  Use your computed variance to determine the *χ*<sup>2</sup> of\n",
    "        the fit. Comment on the value obtained.\n",
    "\n",
    "    7.  Determine the variances *σ*<sub>*a*</sub> and *σ*<sub>*b*</sub>\n",
    "        and check whether it makes sense to use them as the errors in\n",
    "        the deduced values for *a* and *b*.\n",
    "\n",
    "    8.  Now that you have a fit, look at the data again and estimate\n",
    "        what a better value for the errors in the ordinates might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.1  Linear Quadratic Fit<a id=\"7.8.1\"></a>\n",
    "\n",
    "As indicated earlier, as long as the function being fitted depends\n",
    "*linearly* on the unknown parameters *a*<sub>*i*</sub>, the condition of\n",
    "minimum *χ*<sup>2</sup> leads to a set of simultaneous linear equations\n",
    "for the *a*’s that can be solved by hand or on the computer using matrix\n",
    "techniques. To illustrate, suppose we want to fit the quadratic\n",
    "polynomial\n",
    "\n",
    "$$\\tag*{7.57} g(x) = a_1 + a_2x + a_3x^{2}$$\n",
    "\n",
    "to the experimental measurements\n",
    "(*x*<sub>*i*</sub>, *y*<sub>*i*</sub>, *i* = 1, *N*<sub>*D*</sub>)\n",
    "(Figure 7.8). Because this *g*(*x*) is linear in all the parameters\n",
    "*a*<sub>*i*</sub>, we can still make a linear fit although *x* is raised\n",
    "to the second power. \\[However, if we tried to a fit a function of the\n",
    "form\n",
    "*g*(*x*)=(*a*<sub>1</sub> + *a*<sub>2</sub>*x*)exp(−*a*<sub>3</sub>*x*)\n",
    "to the data, then we would not be able to make a linear fit because\n",
    "there is not a linear dependence on *a*<sub>3</sub>.\\]\n",
    "\n",
    "The best fit of this quadratic to the data is obtained by applying the minimum\n",
    "*χ*<sup>2</sup> condition (7.44) for *M*<sub>*p*</sub> = 3 parameters and\n",
    "*N*<sub>*D*</sub> (still arbitrary) data points. A solution represents the\n",
    "maximum likelihood that the deduced parameters provide a correct description\n",
    "of the data for the theoretical function *g*(*x*). Equation (7.44) leads to the\n",
    "three simultaneous equations for *a*<sub>1</sub>, *a*<sub>2</sub>, and\n",
    "*a*<sub>3</sub>:\n",
    "\n",
    "$$\\begin{align}\n",
    " \\tag*{7.58}\n",
    "\\sum_{i=1}^{N_{D}}  \\frac{[y_{i}-g(x_{i})]}{\\sigma_{i}^{2}}\n",
    "\\frac{\\partial g(x_i)} {\\partial a_{1}} & =  0, \\quad\n",
    "\\frac{\\partial g}{\\partial a_1} =1,\\\\\n",
    "\\sum_{i=1}^{N_{D}}  \\frac{[y_{i}-g(x_{i})]}{\\sigma_{i}^{2}}\n",
    "\\frac{\\partial g(x_i)} {\\partial a_{2}} & =  0, \\quad \\frac{\\partial g}{\\partial a_2} =\n",
    "x, \\tag*{7.59}\\\\\n",
    "\\sum_{i=1}^{N_{D}}   \\frac{[y_{i}-g(x_{i})]}{\\sigma_{i}^{2}}\n",
    " \\frac{\\partial g(x_i)} {\\partial a_{3}} & =  0 , \\quad \\frac{\\partial g}{\\partial\n",
    " a_3}=x^2.\\tag*{7.60}\\end{align}$$\n",
    " \n",
    " *Note*: Because the derivatives are independent of the parameters (the\n",
    "*a*’s), the *a* dependence arises only from the term in square brackets\n",
    "in the sums, and because that term has only a linear dependence on the\n",
    "*a*’s, these equations are linear in the *a*’s.\n",
    "\n",
    "**Exercise:** Show that after some rearrangement, (7.58)-(7.60) can be written\n",
    "as \n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.61}\n",
    "S a_1 + S_{x}a_2 + S_{xx}a_3 & = S_{y}, \\\\ S_{x}a_1 + S_{xx} a_2 + S_{xxx} a_3 & =\n",
    "S_{xy}, \\\\ S_{xx} a_1 + S_{xxx} a_2 + S_{xxxx} a_3 & = S_{xxy} .\\end{align}$$\n",
    "\n",
    "Here the definitions of the *S*’s are simple extensions of those used\n",
    "in (7.47)-(7.49) and are programmed in `Fit.py` shown in Listing 7.4.\n",
    "After placing the three unknown parameters into a vector *x* and the\n",
    "known three\n",
    "[*RHS*](http://www.science.oregonstate.edu/~rubin/Books/CPbook/eBook/GlossarySound/rhs.wav)\n",
    "terms in (7.60) into a vector $\\vec{b}$, these equations assume the matrix\n",
    "form:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.62}\n",
    " &{A}\\vec{x}  = \\vec{b},  \\\\\n",
    " &\\mbox{} {A}  = \\left[\\begin{array}{lll}\n",
    "S & S_{x} & S_{xx}\\\\\n",
    " S_{x} & S_{xx} & S_{xxx} \\\\\n",
    " S_{xx} & S_{xxx} & S_{xxxx}\\end{array} \\right]\\!,\\quad\n",
    " \\vec{x} =\n",
    "\\left[\\begin{array}{l}\n",
    "a_1\\\\ a_2\\\\ a_3\\end{array} \\right]\\!,\\quad \\vec{b} =\n",
    "\\left[\\begin{array}{l} S_{y}\\\\ S_{xy}\\\\ S_{xxy}\n",
    "\\end{array} \\right]\\!.\\end{align}$$\n",
    "\n",
    " The solution for the parameter vector $\\vec{a}$ is obtained by solving\n",
    "the matrix equations. Although for 3 × 3 matrices we can write out the\n",
    "solution in closed form, for larger problems the numerical solution\n",
    "requires matrix methods.\n",
    "\n",
    "[**Listing 7.4  Fit.py**](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/Fit.py) performs a least-squares fit of a parabola to data using\n",
    "the NumPy linalg package to solve the set of linear equations ${S}\\vec{a}=\n",
    "\\vec{s}$.\n",
    "\n",
    "**Linear Quadratic Fit Assessment**\n",
    "\n",
    "1.  Fit the quadratic (7.57) to the following data sets \\[given as\n",
    "    (*x*<sub>1</sub>, *y*<sub>1</sub>),(*x*<sub>2</sub>, *y*<sub>2</sub>),…\\].\n",
    "    In each case indicate the values found for the *a*’s, the number of\n",
    "    degrees of freedom, *and* the value of *χ*<sup>2</sup>.\n",
    "\n",
    "    -   (0, 1)\n",
    "\n",
    "    -   (0, 1),(1, 3)\n",
    "\n",
    "    -   (0, 1),(1, 3),(2, 7)\n",
    "\n",
    "    -   (0, 1),(1, 3),(2, 7),(3, 15)\n",
    "\n",
    "2.  Find a fit to the last set of data to the function\n",
    "\n",
    "    $$\\tag*{7.63}\n",
    "        y = A e^{-bx^2}.$$\n",
    "\n",
    "    *Hint:* A judicious change of variables will permit you to convert\n",
    "    this to a linear fit. Does a minimum *χ*<sup>2</sup> still have\n",
    "    meaning here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.2  Problem 5: Nonlinear Fit to a Breit-Wigner<a id=\"7.8.2\"></a>\n",
    "\n",
    "**Problem:** Remember how earlier in this chapter we interpolated the\n",
    "values in Table 7.1 in order to obtain the experimental cross section\n",
    "*Σ* as a function of energy. Although we did not use it, we also gave\n",
    "the theory describing these data, namely, the Breit-Wigner resonance\n",
    "formula (7.21):\n",
    "\n",
    "$$\\tag*{7.64} f(E) = \\frac{f_r}{(E-E_{r})^{2} + \\Gamma^{2}/4}.$$\n",
    "\n",
    "Your **problem** is to determine what values for the parameters\n",
    "*E*<sub>*r*</sub>, *f*<sub>*r*</sub>, and *Γ* in (7.64) provide the best\n",
    "fit to the data in Table 7.1.\n",
    "\n",
    "Because (7.64) is not a linear function of the parameters\n",
    "(*E*<sub>*r*</sub>, *Σ*<sub>0</sub>, *Γ*), the three equations that\n",
    "result from minimizing *χ*<sup>2</sup> are not linear equations and so\n",
    "cannot be solved by the techniques of *linear* algebra (matrix methods).\n",
    "However, in our study of the masses on a string problem we showed how to\n",
    "use the Newton-Raphson algorithm to search for solutions of simultaneous\n",
    "nonlinear equations. That technique involved expansion of the equations\n",
    "about the previous guess to obtain a set of linear equations and then\n",
    "solving the linear equations with the matrix libraries. We now use this\n",
    "same combination of fitting, trial-and-error searching, and matrix\n",
    "algebra to conduct a nonlinear least-squares fit of (7.64) to the data\n",
    "in Table 7.1.\n",
    "\n",
    "Recall that the condition for a best fit is to find values of the\n",
    "*M*<sub>*P*</sub> parameters *a*<sub>*m*</sub> in the theory\n",
    "*g*(*x*, *a*<sub>*m*</sub>) that minimize\n",
    "*χ*<sup>2</sup> = ∑<sub>*i*</sub>\\[(*y*<sub>*i*</sub> − *g*<sub>*i*</sub>)/*σ*<sub>*i*</sub>\\]<sup>2</sup>.\n",
    "This leads to the *M*<sub>*P*</sub> equations (7.44) to solve\n",
    "\n",
    "$$\\tag*{7.65}\n",
    " \\sum_{i=1}^{N_{D}}  \\frac{[y_{i}-g(x_{i})]}{\\sigma_{i}^{2}}\n",
    "\\frac{\\partial g(x_i)} {\\partial a_{m}} = 0, \\quad (m=1,M_{P}).$$\n",
    "\n",
    "To find the form of these equations appropriate to our problem, we\n",
    "rewrite our theory function (7.64) in the notation of (7.65):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.66}\n",
    " a_1 = f_r, \\quad a_2 & =  E_R, \\quad a_3=\\Gamma^2/4,\n",
    "\\quad x = E,\\\\\n",
    "\\Rightarrow \\quad g(x) & =   \\frac{a_1}{(x-a_2)^2+a_3}.\\tag*{7.67}\\end{align}$$\n",
    "\n",
    "The three derivatives required in (7.65) are then\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.68}\n",
    "\\frac{\\partial g}{\\partial a_1}& =  \\frac{1}{(x-a_2)^2+a_3}, \\quad\n",
    "\\frac{\\partial g}{\\partial a_2} =\n",
    "\\frac{-2a_1(x-a_2)}{[(x-a_2)^2+a_3]^2},  \\\\\n",
    "\\quad \\frac{\\partial g}{\\partial a_3}   = &\\frac{-a_1}{[(x-a_2)^2+a_3]^2}.\\end{align}$$\n",
    "\n",
    "Substitution of these derivatives into the best-fit condition (7.65)\n",
    "yields three simultaneous equations in *a*<sub>1</sub>, *a*<sub>2</sub>,\n",
    "and *a*<sub>3</sub> that we need to solve in order to fit the\n",
    "*N*<sub>*D*</sub> = 9 data points (*x*<sub>*i*</sub>, *y*<sub>*i*</sub>)\n",
    "in Table 7.1:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.69}\n",
    "\\sum_{i=1}^9  \\frac{y_i-g(x_i,a)}{(x_i-a_2)^2+a_3} & =  0,\n",
    "\\quad \\sum_{i=1}^9 \\frac{y_i-g(x_i,a)}{[(x_i-a_2)^2+a_3]^2} = 0,  \\\\\n",
    "\\sum_{i=1}^9\n",
    "\\frac{\\left\\{y_i-g(x_i,a)\\right\\}(x_i-a_2)}{[(x_i-a_2)^2+a_3]^2}\n",
    "& = 0.\\end{align}$$\n",
    "\n",
    "Even without the substitution of (7.64) for *g*(*x*, *a*), it is clear\n",
    "that these three equations depend on the *a*’s in a nonlinear fashion.\n",
    "That’s okay because in §6.1.2 we derived the *N*-dimensional\n",
    "Newton-Raphson search for the roots of\n",
    "\n",
    "$$\\tag*{7.70} f_i (a_1, a_2,\\ldots, a_N) = 0 , \\quad i=1,N,$$\n",
    "\n",
    "where we have made the change of variable\n",
    "*y*<sub>*i*</sub> → *a*<sub>*i*</sub> for the present problem. We use\n",
    "that same formalism here for the *N* = 3 equations (7.69) by writing\n",
    "them as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{7.71}\n",
    "f_1(a_1,a_2,a_3) & = \\sum_{i=1}^9\n",
    "\\frac{y_i-g(x_i,a)}{(x_i-a_2)^2+a_3} =0,\\\\\n",
    "f_2(a_1,a_2,a_3) & = \\sum_{i=1}^9\n",
    "\\frac{\\left\\{y_i-g(x_i,a)\\right\\}(x_i-a_2)}{[(x_i-a_2)^2+a_3]^2}\n",
    "=0,\\tag*{7.72}\\\\ f_3(a_1,a_2,a_3) & = \\sum_{i=1}^9 \\frac{y_i-g(x_i,a)}\n",
    "{[(x_i-a_2)^2+a_3]^2}=0.\\tag*{7.73}\\end{align}$$\n",
    "\n",
    "Because *f*<sub>*r*</sub> ≡ *a*<sub>1</sub> is the peak value of the cross\n",
    "section, *E*<sub>*R*</sub> ≡ *a*<sub>2</sub> is the energy at which the\n",
    "peak occurs, and $\\Gamma = 2\\sqrt{a_3}$ is the full width of the peak at\n",
    "half-maximum, good guesses for the *a*’s can be extracted from a graph of the\n",
    "data. To obtain the nine derivatives of the three *f*’s with respect to the three\n",
    "unknown *a*’s, we use two nested loops over *i* and *j*, along with the\n",
    "forward-difference approximation for the derivative\n",
    "\n",
    "$$\\tag*{7.74}\n",
    "\\frac{\\partial f_i}{\\partial a_j} \\simeq \\frac{f_i(a_j + \\Delta\n",
    "a_j) - f_i(a_j)}{\\Delta a_j},$$\n",
    "\n",
    "where *Δa*<sub>*j*</sub> corresponds to a small, say ≤1%, change in the\n",
    "parameter value.\n",
    "\n",
    "**Nonlinear Fit Implementation** Use the Newton-Raphson algorithm as\n",
    "outlined in §7.8.2 to conduct a nonlinear search for the best-fit\n",
    "parameters of the Breit-Wigner theory (7.64) to the data in Table 7.1.\n",
    "Compare the deduced values of\n",
    "(*f*<sub>*r*</sub>, *E*<sub>*R*</sub>, *Γ*) to that obtained by\n",
    "inspection of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
