{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Chapter 3*<br> Errors & Uncertainties in Computations\n",
    "\n",
    "| | | |\n",
    "|:---:|:---:|:---:|\n",
    "| ![image](Figs/Cover.png)|[From **COMPUTATIONAL PHYSICS**, 3rd Ed, 2015](http://physics.oregonstate.edu/~rubin/Books/CPbook/index.html) <br>RH Landau, MJ Paez, and CC Bordeianu (deceased) <br>Copyrights: <br> [Wiley-VCH, Berlin;](http://www.wiley-vch.de/publish/en/books/ISBN3-527-41315-4/) and [Wiley & Sons, New York](http://www.wiley.com/WileyCDA/WileyTitle/productCd-3527413154.html)<br>  R Landau, Oregon State Unv, <br>MJ Paez, Univ Antioquia,<br> C Bordeianu, Univ Bucharest, 2015.<br> Support by National Science Foundation.|![image](Figs/BackCover.png)|\n",
    "\n",
    "> To err is human, to forgive divine. - Alexander Pope\n",
    "\n",
    "**3 Errors & Uncertainties in Computations**<br>\n",
    "[3.1 Types of Errors (Theory)](#3.1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Model for Disaster: Subtractive\n",
    "Cancellation](#3.1.1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Subtractive Cancellation Exercises](#3.1.2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Round-off Errors](#3.1.3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 Round-off Error Accumulation](#3.1.4)<br>\n",
    "[3.2 Error in Bessel Functions (Problem)](#3.2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Numerical Recursion (Method)](#3.2.1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2 Recursion Relations Assessment](#3.2.2)<br>\n",
    "[3.3 Experimental Error Investigation](#3.3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.1 Error Assessment](#3.3.1)<br> \n",
    "\n",
    "*Whether you are careful or not, errors and uncertainties are part of\n",
    "computation. Some errors are the ones that humans inevitably make, but\n",
    "some are introduced by the computer. Computer errors arise because of\n",
    "the limited *precision* with which computers store numbers or because\n",
    "algorithms or models can fail. Although it stifles creativity to keep\n",
    "thinking ’error’ when approaching a computation, it certainly is a waste\n",
    "of time, and possibly harmful, to work with results that are meaningless\n",
    "(’garbage’) because of errors. In this chapter we examine some of the\n",
    "errors and uncertainties that may occur in computations. Although we do\n",
    "not keep repeating a mantra about watching for errors, the lessons of\n",
    "this chapter apply to all other chapters as well.*\n",
    "\n",
    "** This Chapter’s Lecture, Slide Web Links, Applets & Animations**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "|[All Lectures](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/index.html)|[![anything](Figs/RHLlectureMod4.png)](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/index.html)|\n",
    "\n",
    "| *Lecture (Flash)*| *Slides* | *Sections*|\n",
    "|- - -|:- - -:|:- - -:|\n",
    "|[Errors](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Modules/Errors/Errors.html)|[pdf Slides](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/Lectures/Slides/Slides_NoAnimate_pdf/Errors.pdf)| 2.12|\n",
    "\n",
    "## 3.1  Types of Errors (Theory) <a id=\"3.1\"></a>\n",
    "\n",
    "Let us say that you have a program of high complexity. To gauge why\n",
    "errors should be of concern, imagine the program has the logical flow\n",
    "\n",
    "$$\\tag*{3.1}\n",
    "\\mbox{{start}} \\rightarrow U_{1} \\rightarrow U_{2}\n",
    "\\rightarrow \\cdots \\rightarrow U_{n} \\rightarrow\n",
    "\\mbox{{end}},$$\n",
    "\n",
    "where each unit *U* might be a statement or a step. If each unit has probability\n",
    "*p* of being correct, then the joint probability *P* of the whole program being\n",
    "correct is *P* = *p*<sup>*n*</sup>. Let us say we have a medium-sized\n",
    "program with *n* = 1000 steps and that the probability of each step being\n",
    "correct is almost one, *p* ≃ 0.9993. This means that you end up with $P\\simeq\n",
    "\\frac{1}{2}$, that is, a final answer that is as likely wrong as right (not a good\n",
    "way to build a bridge). The <span>problem</span> is that, as a scientist, you\n",
    "want a result that is correct - or at least in which the uncertainty is small and\n",
    "of known size, even if the code executes millions of steps.\n",
    "\n",
    "Four general types of errors exist to plague your computations:\n",
    "\n",
    "**1. Blunders or bad theory:** typographical errors entered with your\n",
    "program or data, running the wrong program or having a fault in your\n",
    "reasoning (theory), using the wrong data file, and so on. (If your\n",
    "blunder count starts increasing, it may be time to go home or take a\n",
    "break.)\n",
    "\n",
    "**2. Random errors:** imprecision caused by events such as fluctuations\n",
    "in electronics, cosmic rays, or someone pulling a plug.These may be\n",
    "rare, but you have no control over them and their likelihood increases\n",
    "with running time; while you may have confidence in a 20 second\n",
    "calculation, a week-long calculation may have to be run multiple times\n",
    "to check reproducibility.\n",
    "\n",
    "**3. Approximation errors:** imprecision arising from simplifying the\n",
    "mathematics so that a problem can be solved on the computer. They include the\n",
    "replacement of infinite series by finite sums, infinitesimal intervals by finite\n",
    "ones, and variable functions by constants. For example, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\sin(x)  & =     \\sum_{n=1}^{\\infty}\n",
    "\\frac{(-1)^{n-1}x^{2n-1}}{(2n-1)!}\n",
    "\\quad\\mbox{(exact)}  \\\\\n",
    "   & \\simeq   \\sum_{n=1}^{N} \\frac{(-1)^{n-1}x^{2n-1}}{(2n-1)!} + {\\cal E}(x,N) \\quad \\mbox{(algorithm),}  \\tag*{3.2}\n",
    "   \\end{align}$$\n",
    "   \n",
    "where ${\\cal E}(x,N)$ is the approximation error and where in this\n",
    "case ${\\cal E}$ is the series from *N* + 1 to ∞. Because approximation error\n",
    "arises from the algorithm we use to approximate the mathematics, it is also\n",
    "called *algorithmic error*. For every reasonable approximation, the\n",
    "approximation error should decrease as *N* increases and should vanish in the\n",
    "limit *N* → ∞. Specifically for (3.2), because the scale for *N* is set by the\n",
    "value of *x*, obtaining a small approximation error requires *N* ≫ *x*. So if *x* and *N*\n",
    "are close in value, the approximation error will be large.\n",
    "\n",
    "**4. Round-off errors:** imprecision arising from the finite number of\n",
    "digits used to store floating-point numbers. These “errors” are\n",
    "analogous to the uncertainty in the measurement of a physical quantity\n",
    "encountered in an elementary physics laboratory. The overall round-off\n",
    "error accumulates as the computer handles more numbers, that is, as the\n",
    "number of steps in a computation increases. This may cause some\n",
    "algorithms to become *unstable* with a rapid increase IN error. In some\n",
    "cases, round-off error may become the major component in your answer,\n",
    "leading to what computer experts call\n",
    "[*garbage.*](http://www.science.oregonstate.edu/~rubin/Books/CPbook/eBook/GlossarySound/garbage.wav)\n",
    "\n",
    "For example, if your computer kept four decimal places, then it will store $\\frac\n",
    "{1}{3}$ as 0.3333 and $\\frac{2}{3}$ as 0.6667, where the computer has\n",
    "“rounded off” the last digit in $\\frac{2}{3}$. Accordingly, if we ask the computer\n",
    "to do as simple a calculation as $2(\\frac{1}{3})-\\frac{2}{3}$, it produces\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{3.3}\n",
    "    2\\left(\\frac{1}{3}\\right) - \\frac{2}{3}\n",
    "    = 0.6666    - 0.6667 = -0.0001 \\neq 0.\n",
    " \\end{align}$$\n",
    "\n",
    "So although the result is small, it is not 0, and if we repeat this type\n",
    "of calculation millions of times, the final answer might not even be\n",
    "small (small garbage begets large garbage).\n",
    "\n",
    "When considering the precision of calculations it is good to recall our\n",
    "discussion in [Chapter 2](CP02.ipynb) of significant figures and of\n",
    "scientific notation given in your early physics or engineering classes.\n",
    "For computational purposes, let us consider how the computer may store\n",
    "the floating-point number\n",
    "\n",
    "$$\\tag*{3.4}\n",
    "    a =11223344556677889900 = 1.12233445566778899 \\times\n",
    "    10^{19}.$$\n",
    "\n",
    "Because the exponent is stored separately and is a small number, we may\n",
    "assume that it will be stored in full precision. In contrast, some of\n",
    "the digits of the mantissa may be truncated. In double precision, the\n",
    "mantissa of *a* will be stored in two words, the *most significant part*\n",
    "representing the decimal 1.12233, and the *least significant part*\n",
    "44556677. The digits beyond 7 are lost. As we shall see soon, when we\n",
    "perform calculations with words of fixed length, it is inevitable that\n",
    "errors will be introduced (at least) into the least significant parts of\n",
    "the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.1  Model for Disaster: Subtractive Cancellation<a id=\"3.1.1\"></a>\n",
    "\n",
    "Calculations employing numbers that are stored only approximately on the\n",
    "computer can only be expected to yield approximate answers. To demonstrate\n",
    "the effect of this type of uncertainty, we model the computer representation\n",
    "*x*<sub>*c*</sub> of the exact number *x* as\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{3.5}\n",
    "  x_c  \\simeq  x (1 +\\epsilon_{x}).\\end{align}$$\n",
    "  \n",
    " Here *ϵ*<sub>*x*</sub> is the relative error in *x*<sub>*c*</sub>,\n",
    "which we expect to be of a similar magnitude to the machine precision\n",
    "*ϵ*<sub>*m*</sub>. If we apply this notation to the simple subtraction\n",
    "*a* = *b* − *c*, we obtain \n",
    "\n",
    "$$\\begin{align}\n",
    "    a &= b - c  \\quad \\Rightarrow \\quad     a_{c}  \\simeq  b_c - c_c \\simeq b(1 + \\epsilon_{b}) - c(1 +\n",
    "    \\epsilon_c)   \\\\\n",
    "   \\mbox{} \\quad &\\Rightarrow \\quad   \\frac{a_c}{a}    \\simeq  1 +\n",
    "    \\epsilon_{b} \\frac{b}{a} - \\frac{c}{a} \\epsilon_c.\\tag*{3.6}\n",
    " \\end{align}$$\n",
    " We see from (3.6) that the resulting error in *a* is essentially a\n",
    "weighted average of the errors in *b* and *c*, with no assurance that the last\n",
    "two terms will cancel. Of special importance here is the observation that the\n",
    "error in the answer *a*<sub>*c*</sub> increases when we subtract two nearly\n",
    "equal numbers (*b* ≃ *c*) because then we are subtracting off the most\n",
    "significant parts of both numbers and leaving the error-prone least-significant\n",
    "parts: \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{a_{c}} {a}      =         1 + \\epsilon_{a}\\simeq 1+\n",
    "\\frac{b}{a}(\\epsilon_{b} - \\epsilon_{c}) \\simeq 1+ \\frac{b}{a}\\;\n",
    "\\mbox{max}(|\\epsilon_{b}|,|\\epsilon_{c}|).\\tag*{3.7}\n",
    "\\end{align}$$\n",
    "\n",
    " This shows that even if the relative errors in *b* and *c* cancel\n",
    "somewhat, they are multiplied by the large number *b*/*a*, which can\n",
    "significantly magnify the error. Because we cannot assume any sign for\n",
    "the errors, we must assume the worst \\[the “max” in (3.6)\\].\n",
    "\n",
    "**Theorem** If you subtract two large numbers and end up with a small\n",
    "one, the small one is less significant than the large numbers.\n",
    "\n",
    "We have already seen an example of subtractive cancellation in the power\n",
    "series summation for sin*x* ≃ *x* − *x*<sup>3</sup>/3!+⋯ for large *x*.A\n",
    "similar effect occurs for\n",
    "*e*<sup>−*x*</sup> ≃ 1 − *x* + *x*<sup>2</sup>/2!−*x*<sup>3</sup>/3!+⋯\n",
    "for large *x*, where the first few terms are large but of alternating\n",
    "sign, leading to an almost total cancellation in order to yield the\n",
    "final small result. (Subtractive cancellation can be eliminated by using\n",
    "the identity *e*<sup>−*x*</sup> = 1/*e*<sup>*x*</sup>, although\n",
    "round-off error will still remain.)\n",
    "\n",
    "### 3.1.2  Subtractive Cancellation Exercises<a id=\"3.1.2\"></a>\n",
    "\n",
    "1.  Remember back in high school when you learned that the quadratic\n",
    "    equation\n",
    "\n",
    "    $$\\tag*{3.8}\n",
    "     a x^{2} + b x + c = 0$$\n",
    "\n",
    "    has an analytic solution that can be written as either[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.9.xml)\n",
    "\n",
    "    $$\\tag*{3.9}\n",
    "    x_{1,2} = \\frac{-b \\pm \\sqrt{b^{2} -4ac}}{2a}\\quad\\mbox{or}\\quad\n",
    "    x_{1,2}' = \\frac{-2c}{b \\pm \\sqrt{b^{2} -4ac}}.$$\n",
    "\n",
    "    Inspection of (3.9) indicates that subtractive cancellation (and\n",
    "    consequently an increase in error) arises when\n",
    "    *b*<sup>2</sup> ≫ 4*a**c* because then the square root and its\n",
    "    preceding term nearly cancel for one of the roots.\n",
    "\n",
    "    -   Write a program that calculates all four solutions for arbitrary\n",
    "        values of *a*, *b*, and *c*.\n",
    "\n",
    "    -   Investigate how errors in your computed answers become large as\n",
    "        the subtractive cancellation increases and relate this to the\n",
    "        known machine precision. (*Hint*: A good test case utilizes\n",
    "        *a* = 1, *b* = 1, *c* = 10<sup>−*n*</sup>, *n* = 1, 2, 3, ….)\n",
    "\n",
    "    -   Extend your program so that it indicates the most\n",
    "        precise solutions.\n",
    "\n",
    "2.  As we have seen, subtractive cancellation occurs when summing a\n",
    "    series with alternating signs. As another example, consider the\n",
    "    finite sum[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.10.xml)\n",
    "\n",
    "    $$\\tag*{3.10}\n",
    "    S_{N}^{(1)} = \\sum_{n=1}^{2N} (-1)^{n} \\frac{n}{n+1}.$$\n",
    "\n",
    "    If you sum the even and odd values of *n* separately, you get two\n",
    "    sums:[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.11.xml)\n",
    "    $$\\tag*{3.11}\n",
    "    S_{N}^{(2)} = - \\sum_{n=1}^{N} \\frac{2n-1}{2n} + \\sum_{n=1}^{N}\n",
    "    \\frac{2n}{2n+1} .$$\n",
    "\n",
    "    All terms are positive in this form with just a single subtraction\n",
    "    at the end of the calculation. Yet even this one subtraction and its\n",
    "    resulting cancellation can be avoided by combining the series\n",
    "    analytically to obtain[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.12.xml)\n",
    "\n",
    "    $$S_{N}^{(3)} = \\sum_{n=1}^{N} \\frac{1}{2n(2n+1)} .\\tag*{3.12}$$\n",
    "\n",
    "    Although all three summations *S*<sup>(1)</sup>, *S*<sup>(2)</sup>,\n",
    "    and *S*<sup>(3)</sup> are mathematically equal, they may give\n",
    "    different numerical results.\n",
    "\n",
    "    -   Write a double-precision program that calculates\n",
    "        *S*<sup>(1)</sup>, *S*<sup>(2)</sup>, and *S*<sup>(3)</sup>.\n",
    "\n",
    "    -   Assume *S*<sup>(3)</sup> to be the exact answer. Make a log-log\n",
    "        plot of the relative error <span>*versus*</span> the number of\n",
    "        terms, that is, of\n",
    "        log<sub>10</sub>|(*S*<sub>*N*</sub><sup>(1)</sup> − *S*<sub>*N*</sub><sup>(3)</sup>)/*S*<sub>*N*</sub><sup>(3)</sup>|\n",
    "        <span>*versus*</span> log<sub>10</sub>(*N*). Start with *N* = 1\n",
    "        and work up to *N* = 1, 000, 000. (Recollect that\n",
    "        log<sub>10</sub>*x* = ln*x*/ln10.) The negative of the ordinate\n",
    "        in this plot gives an approximate value for the number of\n",
    "        significant figures.\n",
    "\n",
    "    -   See whether straight-line behavior for the error occurs in some\n",
    "        region of your plot. This indicates that the error is\n",
    "        proportional to a power of *N*.\n",
    "\n",
    "3.  In spite of the power of your trusty computer, calculating the sum\n",
    "    of even a simple series may require some thought and care. Consider\n",
    "    the two series\n",
    "\n",
    "    $$\\tag*{3.13}\n",
    "    S^{\\textrm (up)} = \\sum_{n=1}^{N} \\frac{1}{n} , \\qquad\n",
    "    S^{\\textrm (down)} = \\sum_{n=N}^{1} \\frac{1}{n} .$$\n",
    "\n",
    "    Both series are finite as long as *N* is finite, and when summed\n",
    "    analytically both give the same answer. Nonetheless, because of\n",
    "    round-off error, the numerical value of $S^{\\textrm (up)}$ will not\n",
    "    be precisely that of $S^{\\textrm (down)}$.\n",
    "\n",
    "    -   Write a program to calculate $S^{\\textrm (up)}$ and\n",
    "        $S^{\\textrm (down)}$ as functions of *N*.\n",
    "\n",
    "    -   Make a log-log plot of\n",
    "        $(S^{\\textrm (up)}-S^{\\textrm (down)})/(|S^{\\textrm (up)}|+\n",
    "        |S^{\\textrm (down)}|)$ <span>*versus*</span> *N*.\n",
    "\n",
    "    -   Observe the linear regime on your graph and explain why the\n",
    "        downward sum is generally more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3  Round-off Errors<a id=\"3.1.3\"></a>\n",
    "\n",
    "Let’s start by seeing how error arises from a single division of the computer\n",
    "representations of two numbers:\n",
    "\n",
    "$$\\begin{align}\n",
    "   a = \\frac{b}{c} \\enspace &\\Rightarrow \\enspace   a_{c} = \\frac{b_{c}} {c_{c}} \\ = \\frac{b(1+\\epsilon_{b})}{c(1 + \\epsilon_{c})},\n",
    "   \\\\\n",
    "\\enspace    &\\Rightarrow \\enspace  \\frac{a_{c}}{a}  =\n",
    "\\frac{1+\\epsilon_{b}}{1 + \\epsilon_{c}}\n",
    " \\simeq    (1 + \\epsilon_{b})(1 - \\epsilon_{c})    \\simeq 1+ \\epsilon_b -\\epsilon_c,\n",
    " \\\\\n",
    "\\enspace        &\\Rightarrow \\enspace\n",
    "   \\frac{a_{c}}{a} \\simeq 1+ |\\epsilon_b| + |\\epsilon_c|.\\tag*{3.14}\n",
    " \\end{align}$$\n",
    " \n",
    " Here we ignore the very small *ϵ*<sup>2</sup> terms and add errors in\n",
    "absolute value because we cannot assume that we are fortunate enough to\n",
    "have unknown errors cancel each other. Because we add the errors in\n",
    "absolute value, this same rule holds for multiplication. Equation (3.14)\n",
    "is just the basic rule of error propagation from elementary laboratory\n",
    "work: You add the uncertainties in each quantity involved in an analysis\n",
    "to arrive at the overall uncertainty.\n",
    "\n",
    "We can even generalize this model to estimate the error in the\n",
    "evaluation of a general function *f*(*x*), that is, the difference in\n",
    "the value of the function evaluated at *x* and at *x*<sub>*c*</sub>:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{3.15}\n",
    "     {\\cal E} = \\frac{f(x)-f(x_c)}{f(x)}\n",
    "     \\simeq \\frac{df(x)/dx}{f(x)} (x-x_c).\n",
    "    \\end{align}$$\n",
    "\n",
    "So, for example,\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{3.16}\n",
    "  f(x) & =  \\sqrt{1+x},   \\qquad     \\frac{df}{dx}\n",
    "   = \\frac{1}{2}\\frac{1}{\\sqrt{1+x}} = \\frac{1}{4}f(x)(x-x_c)\\\\\n",
    "\\Rightarrow \\quad    {\\cal E} & \\simeq     \\frac{1}{2} \\sqrt{1+x}(x-x_c) = \\frac{x-x_c}{2(1+x)}.\\tag*{3.17}\n",
    " \\end{align}$$\n",
    "\n",
    "If we evaluate this expression for *x* = *π*/4 and assume an error in the fourth\n",
    "place of *x*, we obtain a similar relative error of 1.5 × 10<sup>−4</sup> in\n",
    "$\\sqrt{1+x}$.\n",
    "\n",
    "![image](Figs/Fig3_1.png) **Figure 3.1** A schematic of the *N* steps in a\n",
    "random walk simulation that end up a distance $R = \\sqrt{N}$ from the origin.\n",
    "Notice how the *Δ**x*’s for each step add vectorially.\n",
    "\n",
    "### 3.1.4  Round-off Error Accumulation<a id=\"3.1.4\"></a>\n",
    "\n",
    "There is a useful model for approximating how round-off error\n",
    "accumulates in a calculation involving a large number of steps. As\n",
    "illustrated in Figure 3.1, we view the error in each step of a\n",
    "calculation as a literal “step” in a *random walk*, that is, a walk for\n",
    "which each step is in a random direction. As we derive and simulate in\n",
    "[Chapter 4, *Monte Carlo: Randomness, Walks & Decays*](CP04.ipynb), the\n",
    "total distance *R* covered in *N* steps of length *r*, is, on the\n",
    "average,\n",
    "\n",
    "$$\\tag*{3.18}\n",
    "    R \\simeq \\sqrt{N}  r.$$\n",
    "\n",
    "By analogy, the total relative error $\\epsilon_{\\textrm ro}$ arising after *N*\n",
    "calculational steps each with machine precision error *ϵ*<sub>*m*</sub> is, on\n",
    "the average, $$\\tag*{3.19}\n",
    "   \\epsilon_{\\textrm ro} \\simeq \\sqrt{N}      \\epsilon_{m} .$$\n",
    "\n",
    "If the round-off errors in a particular algorithm do not accumulate in a\n",
    "random manner, then a detailed analysis is needed to predict the\n",
    "dependence of the error on the number of steps *N*. In some cases there\n",
    "may be no cancellation, and the error may increase as\n",
    "*N**ϵ*<sub>*m*</sub>. Even worse, in some recursive algorithms, where\n",
    "the error generation is coherent, such as the upward recursion for\n",
    "spherical Bessel functions, there may be an *N*! increase in error.\n",
    "\n",
    "## 3.2  Error in Bessel Functions (Problem) <a id=\"3.2\"></a>\n",
    "\n",
    "Accumulating round-off errors often limits the ability of a program to\n",
    "calculate accurately.Your **problem** is to compute the spherical Bessel\n",
    "and Neumann functions *j*<sub>*l*</sub>(*x*) and *n*<sub>*l*</sub>(*x*).\n",
    "These function are, respectively, the regular/irregular\n",
    "(nonsingular/singular at the origin) solutions of the differential\n",
    "equation\n",
    "\n",
    "$$\\tag*{3.20}\n",
    "    x^{2} f\"(x) + 2 x f'(x) + \\left[x^{2} - l(l+1)\\right]f(x) = 0 .$$\n",
    "\n",
    "The spherical Bessel functions are related to the Bessel function of the first kind\n",
    "by $j_l(x) = \\sqrt{\\pi/2x}J_{n+1/2}(x)$. They occur in many physical problems,\n",
    "such as the expansion of a plane wave into spherical partial waves,\n",
    "\n",
    "$$e^{i\\textbf{k} \\cdot \\textbf{r}} = \\sum_{l=0}^{\\infty} i^{l} (2l+1) j_{l}(kr)\n",
    "P_{l}(\\cos \\ \\theta).\\tag*{3.21}$$\n",
    "\n",
    "Figure 3.2 shows what the first few *j*<sub>*l*</sub> look like, and\n",
    "Table 3.1 gives some explicit values. For the first two *l* values,\n",
    "explicit forms are\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tag*{3.22}\n",
    "    j_0(x) & =  +\\frac{\\sin x}{x}, \\qquad j_1(x) =\n",
    "    +\\frac{\\sin x}{x^2}-\\frac{\\cos x}{x}\\\\\n",
    "n_0(x) & = -\\frac{\\cos x}{x}. \\qquad n_1(x)= -\\frac{\\cos x}{x^2}-\\frac{\\sin\n",
    "x}{x}.\\tag*{3.23}\n",
    "    \\end{align}$$\n",
    "\n",
    "![image](Figs/Fig3_2.png) **Figure 3.2** The first four spherical Bessel\n",
    "functions *j*<sub>*l*</sub> (*x*) as functions of *x*. Notice that for\n",
    "small *x*, the values for increasing *l* become progressively smaller.\n",
    "\n",
    "### 3.2.1  Numerical Recursion (Method)<a id=\"3.2.1\"></a>\n",
    "\n",
    "The classic way to calculate *j*<sub>*l*</sub>(*x*) would be by summing its\n",
    "power series for small values of *x*/*l* and summing its asymptotic expansion\n",
    "for large *x*/*l* values. The approach we adopt is based on the *recursion\n",
    "relations*[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.24.xml) \n",
    "\n",
    "$$\\begin{align}\n",
    " j_{l+1}(x) &= \\frac{2l+1}{x} j_{l}(x) -\n",
    "    j_{l-1}(x), \\quad \\mbox{(up)}, \\tag*{3.24}\\\\\n",
    "    j_{l-1}(x) &= \\frac{2l+1}{x} j_{l}(x) -\n",
    "    j_{l+1}(x), \\quad \\mbox{(down)}.\\tag*{3.25}\n",
    " \\end{align}$$\n",
    "\n",
    "Equations (3.24) and (3.25) are the same relation, one written for\n",
    "upward recurrence from small to large *l* values, and the other for\n",
    "downward recurrence from large *l* to small *l*. With just a few\n",
    "additions and multiplications, recurrence relations permit rapid, simple\n",
    "computation of the entire set of *j*<sub>*l*</sub> values for fixed *x*\n",
    "and all *l*.\n",
    "\n",
    "To recur upward in *l* for fixed *x*, we start with the known forms for\n",
    "*j*<sub>0</sub> and *j*<sub>1</sub> (3.22), and use (3.24). As you will\n",
    "prove for yourself, this upward recurrence usually seems to work at\n",
    "first but then fails. The reason for the failure can be seen from the\n",
    "plots of *j*<sub>*l*</sub>(*x*) and *n*<sub>*l*</sub>(*x*)\n",
    "<span>*versus*</span> *x* (Figure 3.2). If we start at *x* ≃ 2 and\n",
    "*l* = 0, we will see that as we recur *j*<sub>*l*</sub> up to larger *l*\n",
    "values with (3.24), we are essentially taking the difference of two\n",
    "“large” functions to produce a “small” value for *j*<sub>*l*</sub>. This\n",
    "process suffers from subtractive cancellation and always reduces the\n",
    "precision. As we continue recurring, we take the difference of two small\n",
    "functions with large errors and produce a smaller function with yet a\n",
    "larger error. After a while, we are left with only round-off error\n",
    "(garbage).\n",
    "\n",
    "To be more specific, let us call *j*<sub>*l*</sub><sup>(*c*)</sup> the\n",
    "numerical value we compute as an approximation for\n",
    "*j*<sub>*l*</sub>(*x*). Even if we start with pure *j*<sub>*l*</sub>,\n",
    "after a short while the computer’s lack of precision effectively mixes\n",
    "in a bit of *n*<sub>*l*</sub>(*x*):\n",
    "\n",
    "$$\\tag*{3.26}\n",
    "    j_l^{(c)} = j_{l}(x) + \\epsilon n_{l}(x).$$\n",
    "\n",
    "This is inevitable because both *j*<sub>*l*</sub> and *n*<sub>*l*</sub>\n",
    "satisfy the same differential equation and, on that account, the same\n",
    "recurrence relation. The admixture of *n*<sub>*l*</sub> becomes a\n",
    "problem when the numerical value of *n*<sub>*l*</sub>(*x*) is much\n",
    "larger than that of *j*<sub>*l*</sub>(*x*) because even a minuscule\n",
    "amount of a very large number may be large.\n",
    "\n",
    "The simple solution to this problem (*Miller’s device*) is to use (3.25)\n",
    "for downward recursion of the *j*<sub>*l*</sub> values starting at a\n",
    "large value *l* = *L*. This avoids subtractive cancellation by taking\n",
    "small values of *j*<sub>*l* + 1</sub>(*x*) and *j*<sub>*l*</sub>(*x*)\n",
    "and producing a larger *j*<sub>*l* − 1</sub>(*x*) by addition. While the\n",
    "error may still behave like a Neumann function, the actual magnitude of\n",
    "the error will *decrease* quickly as we move downward to smaller *l*\n",
    "values. In fact, if we start iterating downward with arbitrary values\n",
    "for *j*<sub>*L* + 1</sub><sup>(*c*)</sup> and\n",
    "*j*<sub>*L*</sub><sup>(*c*)</sup>, after a short while we will arrive at\n",
    "the correct *l* dependence for this value of *x*. Although the precise\n",
    "value of *j*<sub>0</sub><sup>(*c*)</sup> so obtained will not be correct\n",
    "because it depends upon the arbitrary values assumed for\n",
    "*j*<sub>*L* + 1</sub><sup>(*c*)</sup> and\n",
    "*j*<sub>*L*</sub><sup>(*c*)</sup>, the relative values will be accurate.\n",
    "The absolute values are fixed from the known value (3.22),\n",
    "*j*<sub>0</sub>(*x*)=sin*x*/*x*. Because the recurrence relation is a\n",
    "linear relation between the *j*<sub>*l*</sub> values, we need only\n",
    "normalize all the computed values via\n",
    "\n",
    "$$\\tag*{3.27} j_l^{\\rm N}(x) = j_l^{\\rm c}(x) \\times\n",
    "\\frac{j_0^{\\rm anal}(x)}{j_0^{\\rm c}(x)}.$$\n",
    "\n",
    "Accordingly, after you have finished the downward recurrence, you obtain\n",
    "the final answer by normalizing all *j*<sub>*l*</sub><sup>(*c*)</sup>\n",
    "values based on the known value for *j*<sub>0</sub>.\n",
    "\n",
    "**Table 3.1** Approximate Values for Spherical Bessel Functions (from\n",
    "Maple)\n",
    "\n",
    "|*x* | *j*<sub>3</sub>(*x*) | *j*<sub>5</sub>(*x*)|*j*<sub>8</sub>(*x*)|\n",
    "|- - -|:- - -:|:- - -:|:- - -:|\n",
    "|0.1 |+9.51851971910<sup>−6</sup> |+9.61631023110<sup>−10</sup>|+2.90120010210<sup>−16</sup>|\n",
    "|1 | +9.00658111810<sup>−3</sup> |+9.25611586210<sup>−05</sup>|+2.82649880210<sup>−08</sup>|\n",
    "|10 | −3.94958449810<sup>−2</sup> |−5.55345116210<sup>−02</sup>|+1.25578023610<sup>−01</sup>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2  Implementation and Assessment: Recursion Relations<a id=\"3.2.2\"></a>\n",
    "\n",
    "A program implementing recurrence relations is most easily written using\n",
    "subscripts. If you need to polish up on your skills with subscripts, you\n",
    "may want to study our program [`Bessel.py`](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/Bessel.py) in Listing 3.1 before writing\n",
    "your own.\n",
    "\n",
    "1.  Write a program that uses both upward and downward recursion to\n",
    "    calculate *j*<sub>*l*</sub>(*x*) for the first 25 *l* values for\n",
    "    *x* = 0.1, 1 and 10.\n",
    "\n",
    "2.  Tune your program so that at least one method gives “good” values\n",
    "    (meaning a relative error ≃10<sup>−10</sup>). See Table 3.1 for some\n",
    "    sample values.\n",
    "\n",
    "3.  Show the convergence and stability of your results.\n",
    "\n",
    "4.  Compare the upward and downward recursion methods, printing out *l*,\n",
    "    *j*<sub>*l*</sub><sup>(*u**p*)</sup>,\n",
    "    *j*<sub>*l*</sub><sup>(*d**o**w**n*)</sup>, and the relative\n",
    "    difference\n",
    "    |*j*<sub>*l*</sub><sup>(*u**p*)</sup> − *j*<sub>*l*</sub><sup>(*d**o**w**n*)</sup>|/(|*j*<sub>*l*</sub><sup>(*u**p*)</sup>|+|*j*<sub>*l*</sub><sup>(*d**o**w**n*)</sup>|).\n",
    "\n",
    "5.  The errors in computation depend on *x*, and for certain values of\n",
    "    *x*, both up and down recursions give similar answers. Explain the\n",
    "    reason for this.\n",
    "\n",
    "[**Listing 3.1  Bessel.py**](http://www.science.oregonstate.edu/~rubin/Books/CPbook/Codes/PythonCodes/Bessel.py) determines spherical Bessel functions by\n",
    "downward recursion (you should modify this to also work by upward\n",
    "recursion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Bessel.py, Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bessel.py, Notebook Version\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Xmax = 40.25\n",
    "Xmin = 0.25\n",
    "step = 0.1   # Global class variables\n",
    "order = 10; start = 50        # Plot j_order\n",
    "j0=np.zeros((400),float)\n",
    "j0=np.zeros((400),float)\n",
    "j1=np.zeros((400),float)\n",
    "j2=np.zeros((400),float)\n",
    "j3=np.zeros((400),float)\n",
    "xx=np.zeros((400),float)\n",
    "\n",
    "def down (x, n, m):        # Method down, recurs downward\n",
    "    j = np.zeros( (start  +  2), float)\n",
    "    j[m+ 1] = j[m] = 1.   # Start with anything\n",
    "    for k in range(m, 0,  - 1):\n",
    "        j[k - 1] = ((2.*k + 1.)/x)*j[k]-j[k + 1]\n",
    "    scale = (np.sin(x)/x)/j[0]  # Scale solution to known j[0]\n",
    "    return j[n] * scale\n",
    "\n",
    "i=0\n",
    "for x in np.arange(Xmin, Xmax, step):\n",
    "    xx[i]=x  \n",
    "    j0[i]= down(x, 0, start)\n",
    "    j1[i]=down(x,1,start)\n",
    "    j2[i]=down(x,2,start)\n",
    "    j3[i]=down(x,3,start)  \n",
    "    i+=1   \n",
    "lines=plt.plot(xx,j0,'r',xx,j1,'g',xx,j2,'b',xx,j3,'c')\n",
    "plt.legend( ('j0','j1','j2','j3'),loc='upper right')\n",
    "plt.title(\"First Four Spherical Bessel Functions\")\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3  Experimental Error Investigation <a id=\"3.3\"></a>\n",
    "\n",
    "Numerical algorithms play a vital role in computational physics. Your\n",
    "**problem** is to take a general algorithm and decide\n",
    "\n",
    "1.  Does it converge?\n",
    "\n",
    "2.  How precise are the converged results?\n",
    "\n",
    "3.  How expensive (time-consuming) is it?\n",
    "\n",
    "On first thought you may think, “What a dumb problem! All algorithms\n",
    "converge if enough terms are used, and if you want more precision, then\n",
    "use more terms.” Well, some algorithms may be asymptotic expansions that\n",
    "just approximate a function in certain regions of parameter space and\n",
    "converge only up to a point. Yet even if a uniformly convergent power\n",
    "series is used as the algorithm, including more terms will decrease the\n",
    "algorithmic error but will also increase the round-off errors. And\n",
    "because round-off errors eventually diverge to infinity, the best we can\n",
    "hope for is a “best” approximation. Good algorithms are good not only\n",
    "because they are fast but also because they require fewer steps and thus\n",
    "incur less round-off error.\n",
    "\n",
    "Let us assume that an algorithm takes *N* steps to find a good answer.\n",
    "As a rule of thumb, the approximation (algorithmic) error decreases\n",
    "rapidly, often as the inverse power of the number of terms used:\n",
    "\n",
    "$$\\tag*{3.28}\n",
    "    \\epsilon_{\\textrm app} \\simeq   \\frac{\\alpha}{N^{\\beta}}.$$\n",
    "\n",
    "Here *α* and *β* are empirical constants that change for different\n",
    "algorithms and may be only approximately constant, and even then only as\n",
    "*N* → ∞. The fact that the error must fall off for large *N* is just a\n",
    "statement that the algorithm converges.\n",
    "\n",
    "In contrast to algorithmic error, round-off error grows slowly and somewhat\n",
    "randomly with *N*. If the round-off errors in each step of the algorithm are not\n",
    "correlated, then we know from previous discussion that we can model the\n",
    "accumulation of error as a random walk with step size equal to the machine\n",
    "precision *ϵ*<sub>*m*</sub>: \n",
    "\n",
    "    $$\\begin{align}\n",
    "\\tag*{3.29}\n",
    "    \\epsilon_{\\textrm ro} \\simeq \\sqrt{N} \\epsilon_{m}.\\end{align}$$\n",
    "    \n",
    " This is the slow growth with *N* that we expect from round-off error.\n",
    "The total error in a computation is the sum of the two types of errors:\n",
    "    \n",
    "$$\\begin{align}\n",
    "\\tag*{3.30}\n",
    "  &  \\epsilon_{\\textrm tot}   =     \\epsilon_{\\textrm app}\n",
    "   + \\epsilon_{\\textrm ro}   \\\\\n",
    "  &\\epsilon_{\\textrm tot}   \\simeq  \\frac{\\alpha}{N^{\\beta}} + \\sqrt{N} \\epsilon_{m}\n",
    "  .\\tag*{3.31}\\end{align}$$\n",
    "  \n",
    " For small *N* we expect the first term to be the larger of the two, but\n",
    "as *N* grows it will be overcome by the growing round-off error.\n",
    "\n",
    "![image](Figs/Fig3_3B.png) **Figure 3.3** A log-log plot of relative\n",
    "error *versus* the number of points used for a numerical integration.\n",
    "The ordinate value of $\\sim 10^{-14}$ at the minimum indicates that\n",
    "∼14 decimal places of precision are obtained before round-off error\n",
    "begins to build up. Notice that while the round-off error does fluctuate\n",
    "indicating a statistical aspect of error accumulation, on the average it\n",
    "is increasing but more slowly than did the algorithm’s error decrease.\n",
    "\n",
    "As an example, in Figure 3.3 we present a log-log plot of the relative error in\n",
    "numerical integration using the Simpson integration rule\n",
    "([Chapter 5](CP05.ipynb)). We use the log<sub>10</sub> of the relative error\n",
    "because its negative tells us the number of decimal places of precision obtained.\n",
    "[*Note:* Most computer languages use ln*x* = log<sub>*e*</sub>*x*. Yet\n",
    "because *x* = *a*<sup>log<sub>*a*</sub>​*x*</sup>, we have\n",
    "log<sub>10</sub>*x* = ln*x*/ln10.\\] Let us assume 𝒜 is the exact answer and\n",
    "*A*(*N*) the computed answer. If\n",
    "\n",
    "$$\\tag*{3.32}\n",
    " \\frac{{\\cal A} -A(N)} {{\\cal A}    } \\simeq  10^{-9}, \\quad\n",
    " \\mbox{then} \\quad \\log_{10}\\left| \\frac{{\\cal A} -A(N)} { {\\cal A} }\\right|\n",
    "  \\simeq  -9.$$\n",
    "\n",
    "We see in Figure 3.3 that the error does show a rapid decrease for small\n",
    "*N*, consistent with an inverse power law (3.28). In this region the\n",
    "algorithm is converging. As *N* keeps increasing, the error starts to\n",
    "look somewhat erratic, with a slow increase on the average. In\n",
    "accordance with (3.30), in this region round-off error has grown larger\n",
    "than the approximation error and will continue to grow for increasing\n",
    "*N*.Clearly then, the smallest total error will be obtained if we can\n",
    "stop the calculation at the minimum near 10<sup>−14</sup>, that is, when\n",
    "*ϵ*<sub>approx</sub> ≃ *ϵ*<sub>ro</sub>.\n",
    "\n",
    "In realistic calculations you would not know the exact answer; after\n",
    "all, if you did, then why would you bother with the computation?\n",
    "However, you may know the exact answer for a similar calculation, and\n",
    "you can use that similar calculation to perfect your numerical\n",
    "technique. Alternatively, now that you understand how the total error in\n",
    "a computation behaves, you should be able to look at a table or, better\n",
    "yet, a graph like Figure 3.3, of your answer and deduce the manner in\n",
    "which your algorithm is converging. Specifically, at some point you\n",
    "should see that the mantissa of the answer changes only in the less\n",
    "significant digits, with that place moving further to the right of the\n",
    "decimal point as the calculation executes more steps. Eventually,\n",
    "however, as the number of steps becomes even larger, round-off error\n",
    "leads to a fluctuation in the less significant digits, with a gradual\n",
    "move toward the decimal point. It is best to quit the calculation before\n",
    "this occurs.\n",
    "\n",
    "Based upon this understanding, an approach to obtaining the best\n",
    "approximation is to deduce when your answer behaves like (3.30). To do\n",
    "that, we call 𝒜 the exact answer and *A*(*N*) the computed answer after\n",
    "*N* steps. We assume that for large enough values of *N*, the\n",
    "approximation converges as\n",
    "\n",
    "$$\\tag*{3.33}\n",
    "    A(N)  \\simeq \\mathcal{A} + \\frac{\\alpha}{N^{\\beta}},$$\n",
    "\n",
    "that is, that the round-off error term in (3.30) is still small. We then run our\n",
    "computer program with 2*N* steps, which should give a better answer, and use\n",
    "that answer to eliminate the unknown ${\\cal A}$:\n",
    "\n",
    "$$\\tag*{3.34}\n",
    " A(N) - A(2N)  \\simeq    \\frac{\\alpha}{N^{\\beta}} .$$\n",
    "\n",
    "To see if these assumptions are correct and determine what level of\n",
    "precision is possible for the best choice of *N*, plot\n",
    "log<sub>10</sub>|\\[*A*(*N*)−*A*(2*N*)\\]/*A*(2*N*)| <span>*versus*</span>\n",
    "log<sub>10</sub>*N*, similar to what we have performed in Figure 3.3. If\n",
    "you obtain a rapid straight-line drop off, then you know you are in the\n",
    "region of convergence and can deduce a value for *β* from the slope. As\n",
    "*N* gets larger, you should see the graph change from a straight-line\n",
    "decrease to a slow increase as round-off error begins to dominate. A\n",
    "good place to quit is before this. In any case, now you understand the\n",
    "error in your computation and therefore have a chance to control it.\n",
    "\n",
    "As an example of how different kinds of errors enter into a computation, we\n",
    "assume we know the analytic form for the approximation and round-off errors:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\epsilon_{\\textrm app} & \\simeq \\frac{1}{N^{2}},\\tag*{3.35}\\\\\n",
    "    \\epsilon_{\\textrm ro}  & \\simeq \\sqrt{N}\\epsilon_m,\n",
    "    \\tag*{3.36}\\\\\n",
    "   \\Rightarrow \\enspace     \\epsilon_{\\text{tot}} &= \\epsilon_{\\text{approx}} + \\epsilon_{\\text{ro}}\\tag*{3.37}\\\\\n",
    "    &  \\simeq \\frac{1}{N^{2}} +  \\sqrt{N}\\epsilon_m.\\tag*{3.38}\\end{align}$$\n",
    " The total error is then a minimum when\n",
    "$$\\begin{align}\n",
    "\\tag*{3.39}\n",
    "    \\frac{d\\epsilon_{\\text{tot}}}{dN} &= \\frac{-2}{N^3} + \\frac{1}{2}\\frac{\\epsilon_m}{\\sqrt{N}} =\n",
    "    0,\\\\\n",
    "      \\Rightarrow \\enspace  N^{5/2} &=    \\frac{4}{\\epsilon_{m}}.\\tag*{3.40}\\end{align}$$\n",
    "      \n",
    " For a double-precision calculation\n",
    "(*ϵ*<sub>*m*</sub> ≃ 10<sup>−15</sup>), the minimum total error occurs\n",
    "when\n",
    "\n",
    "$$N^{5/2} \\simeq \\frac{4}{ 10^{-15}}\n",
    " \\enspace \\Rightarrow \\enspace\n",
    "N \\simeq 1099 , \\enspace \\Rightarrow \\enspace\n",
    "    \\epsilon_{\\textrm tot}    \\simeq        4 \\times 10^{-6}.\\tag*{3.41}$$\n",
    "\n",
    "In this case most of the error is as a result of round-off and is not\n",
    "approximation error.\n",
    "\n",
    "Seeing that the total error is mainly round-off error ${\\propto}\\sqrt{N}$, an\n",
    "obvious way to decrease the error is to use a smaller number of steps *N*. Let\n",
    "us assume we do this by finding another algorithm that converges more rapidly\n",
    "with *N*, for example, one with approximation error behaving like\n",
    "\n",
    "$$\\tag*{3.42}\n",
    "    \\epsilon_{\\textrm app} \\simeq \\frac{2}{N^{4}} .$$\n",
    "The total error is now\n",
    "$$\\tag*{3.43}\n",
    "    \\epsilon_{\\textrm tot} = \\epsilon_{\\textrm ro}+\\epsilon_{\\textrm app} \\simeq      \\frac{2}{N^{4}} + \\sqrt{N} \\epsilon_{m}.$$\n",
    "\n",
    "The number of points for minimum error is found as before:\n",
    "\n",
    "$$\\tag*{3.44}\n",
    "    \\frac{d\\epsilon_{\\text{tot}}}{dN} = 0         \\enspace   \\Rightarrow \\enspace  N^{{9}/{2}}\n",
    "  \\enspace \\Rightarrow \\enspace        N \\simeq 67  \\enspace \\Rightarrow \\enspace\n",
    "    \\epsilon_{\\text{tot}}       \\simeq 9\n",
    "\\times 10^{-7}.$$\n",
    "\n",
    "The error is now smaller by a factor of 4, with only 1/16 as many steps\n",
    "needed. Subtle are the ways of the computer. In this case the better\n",
    "algorithm is quicker and, by using fewer steps, produces less round-off\n",
    "error.\n",
    "\n",
    "**Exercise:** Estimate the error now for a double-precision calculation.\n",
    "\n",
    "### 3.3.1  Error Assessment<a id=\"3.3.1\"></a>\n",
    "\n",
    "![image](Figs/Fig3_4.png) **Figure 3.4** The error in the summation of\n",
    "the series for *e*<sup>−*x*</sup> *versus* *N* for various *x* values.\n",
    "The values of *x* increase vertically for each curve. Note that a\n",
    "negative initial slope corresponds to decreasing error with *N*, and\n",
    "that the dip corresponds to a rapid convergence followed by a rapid\n",
    "increase in error. (Courtesy of J. Wiren.)\n",
    "\n",
    "In § 2.5 we have already discussed the Taylor expansion of sin*x*: \n",
    "\n",
    "$$\\tag*{3.45}\n",
    "\\sin(x) = x - \\frac{x^{3}}{3 !} + \\frac{x^{5}}{5!} -\n",
    "\\frac{x^{7}}{7!} +   \\cdots =\\sum_{n=1}^\\infty\n",
    "\\frac{(-1)^{n-1}x^{2n-1}} { (2n-1)!}.$$\n",
    "\n",
    "We now extend that discussion with errors in mind . The series (3.45)\n",
    "converges in the mathematical sense for all values of *x*. Accordingly,\n",
    "a reasonable algorithm to compute the sin(*x*) might be\n",
    "\n",
    "$$\\tag*{3.46}\n",
    "    \\sin(x)  \\simeq \\sum_{n=1}^N  \\frac{(-1)^{n-1}x^{2n-1}} { (2n-1)!}.$$\n",
    "\n",
    "1.  Write a program that calculates sin(*x*) as the finite sum (3.46).\n",
    "    (If you already did this in [Chapter 2](CP02.ipynb), then you may\n",
    "    reuse that program and its results here. But remember, you should\n",
    "    not be using factorials in the algorithm.)\n",
    "\n",
    "2.  Calculate your series for *x* ≤ 1 and compare it to the built-in\n",
    "    function `Math.sin(x)` (you may assume that the built-in function\n",
    "    is exact). Stop your summation at an *N* value for which the next\n",
    "    term in the series will be no more than 10<sup>−7</sup> of the sum\n",
    "    up to that point,[[xml]](http://physics.oregonstate.edu/~rubin/Books/CPbook/eBook/xml/3.47.xml)\n",
    "\n",
    "    $$\\tag*{3.47}\n",
    "    \\frac{|(-1)^N x^{2N+1}|} { (2N-1)!} \\le  10^{-7}  \\left\n",
    "    |\\sum_{n=1}^N \\frac{(-1)^{n-1}x^{2n-1}} { (2n-1)!}\\right|.$$\n",
    "\n",
    "3.  Examine the terms in the series for *x* ≃ 3*π* and observe the\n",
    "    significant subtractive cancellations that occur when large terms\n",
    "    add together to give small answers. \\[Do not use the identity\n",
    "    sin(*x* + 2*π*)=sin*x* to reduce the value of *x* in the series.\\]\n",
    "    In particular, print out the near-perfect cancellation around\n",
    "    *n* ≃ *x*/2.\n",
    "\n",
    "4.  See if better precision is obtained by using trigonometric\n",
    "    identities to keep 0 ≤ *x* ≤ *π*.\n",
    "\n",
    "5.  By progressively increasing *x* from 1 to 10, and then from 10 to\n",
    "    100, use your program to determine experimentally when the series\n",
    "    starts to lose accuracy and when it no longer converges.\n",
    "\n",
    "6.  Make a series of graphs of the error *versus* *N* for different\n",
    "    values of *x* . You should get curves similar to those\n",
    "    in Figure 3.4.\n",
    "\n",
    "Because this series summation is such a simple, correlated process, the\n",
    "round-off error does not accumulate randomly as it might for a more\n",
    "complicated computation, and we do not obtain the error behavior (3.33).\n",
    "We will see the predicted error behavior when we examine integration\n",
    "rules in [Chapter 5](CP05.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
